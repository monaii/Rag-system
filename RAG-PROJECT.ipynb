{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69df0622-d7a1-43eb-82b2-b72576c1e701",
   "metadata": {},
   "source": [
    "# ðŸ“„ Project Introduction\n",
    "\n",
    "This project implements a lightweight and modular AI framework for the **automatic search and validation of documentary content** using a Retrieval-Augmented Generation (RAG) pipeline.\n",
    "\n",
    "It is designed for corporate environments where policies, procedures, technical manuals, and compliance documents must be checked for the presence of **specific requirements** (e.g., ISO 27001 controls).\n",
    "\n",
    "### âœ” Core capabilities\n",
    "- **Document ingestion** from PDF, DOCX, PPTX, and TXT  \n",
    "- **Semantic chunking + embeddings** using OpenAI or fully local LLMs (Ollama)  \n",
    "- **Persistent vector database** (Chroma) for fast semantic retrieval  \n",
    "- **RAG-based question answering**, strictly grounded in retrieved context  \n",
    "- **Requirement validation module** that returns:  \n",
    "  - *present / not present / uncertain*  \n",
    "  - justification  \n",
    "  - evidence (source chunks)  \n",
    "- **Hallucination mitigation** using a secondary verification step  \n",
    "- **Optional LangGraph workflow**, modeling the pipeline as nodes  \n",
    "  (retrieve â†’ generate â†’ verify)\n",
    "\n",
    "###  Goal\n",
    "The goal is to demonstrate how modern LLMs + semantic search can **automate manual document review**, reduce human error, and provide **consistent, auditable compliance checking** in corporate settings.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "967dd1a4-bb3b-46f9-b807-8a525304ef1c",
   "metadata": {},
   "source": [
    "# ðŸ”§ 0. Environment Setup\n",
    "\n",
    "Before running the RAG Document Validation framework, we install the required\n",
    "Python libraries.\n",
    "\n",
    "This project uses:\n",
    "\n",
    "### âœ” Core RAG Components\n",
    "- **LangChain** (retrievers, prompt templates, chains)\n",
    "- **ChromaDB** (persistent vector store)\n",
    "- **Ollama** or **OpenAI** embeddings and chat models\n",
    "\n",
    "### âœ” Workflow Orchestration (Optional)\n",
    "- **LangGraph** for modeling the RAG pipeline as a graph  \n",
    "  (retrieve â†’ generate â†’ verify)\n",
    "\n",
    "### âœ” Utility\n",
    "- **python-dotenv** for loading `.env` configuration\n",
    "\n",
    "All dependencies are included in `requirements.txt`.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c160bed7-336e-45c8-94cf-49917d735807",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-dotenv in /opt/anaconda3/lib/python3.13/site-packages (from -r requirements.txt (line 1)) (1.1.0)\n",
      "Requirement already satisfied: flask in /opt/anaconda3/lib/python3.13/site-packages (from -r requirements.txt (line 2)) (3.1.0)\n",
      "Requirement already satisfied: langchain in /opt/anaconda3/lib/python3.13/site-packages (from -r requirements.txt (line 3)) (0.3.7)\n",
      "Requirement already satisfied: langchain-community in /opt/anaconda3/lib/python3.13/site-packages (from -r requirements.txt (line 4)) (0.3.7)\n",
      "Requirement already satisfied: langchain-core in /opt/anaconda3/lib/python3.13/site-packages (from -r requirements.txt (line 5)) (0.3.63)\n",
      "Requirement already satisfied: langchain-openai in /opt/anaconda3/lib/python3.13/site-packages (from -r requirements.txt (line 6)) (0.3.19)\n",
      "Requirement already satisfied: langchain-chroma in /opt/anaconda3/lib/python3.13/site-packages (from -r requirements.txt (line 7)) (0.2.2)\n",
      "Requirement already satisfied: langchain-mistralai in /opt/anaconda3/lib/python3.13/site-packages (from -r requirements.txt (line 8)) (0.2.10)\n",
      "Requirement already satisfied: langchain-pinecone in /opt/anaconda3/lib/python3.13/site-packages (from -r requirements.txt (line 9)) (0.2.13)\n",
      "Requirement already satisfied: langchain-ollama in /opt/anaconda3/lib/python3.13/site-packages (from -r requirements.txt (line 10)) (0.3.3)\n",
      "Requirement already satisfied: chromadb in /opt/anaconda3/lib/python3.13/site-packages (from -r requirements.txt (line 11)) (0.6.3)\n",
      "Requirement already satisfied: faiss-cpu in /opt/anaconda3/lib/python3.13/site-packages (from -r requirements.txt (line 12)) (1.13.0)\n",
      "Requirement already satisfied: pinecone-client in /opt/anaconda3/lib/python3.13/site-packages (from -r requirements.txt (line 13)) (6.0.0)\n",
      "Requirement already satisfied: pypdf in /opt/anaconda3/lib/python3.13/site-packages (from -r requirements.txt (line 14)) (6.4.0)\n",
      "Requirement already satisfied: docx2txt in /opt/anaconda3/lib/python3.13/site-packages (from -r requirements.txt (line 15)) (0.9)\n",
      "Requirement already satisfied: unstructured in /opt/anaconda3/lib/python3.13/site-packages (from -r requirements.txt (line 16)) (0.18.21)\n",
      "Requirement already satisfied: python-pptx in /opt/anaconda3/lib/python3.13/site-packages (from -r requirements.txt (line 17)) (1.0.2)\n",
      "Requirement already satisfied: langgraph in /opt/anaconda3/lib/python3.13/site-packages (from -r requirements.txt (line 18)) (0.5.4)\n",
      "Requirement already satisfied: Werkzeug>=3.1 in /opt/anaconda3/lib/python3.13/site-packages (from flask->-r requirements.txt (line 2)) (3.1.3)\n",
      "Requirement already satisfied: Jinja2>=3.1.2 in /opt/anaconda3/lib/python3.13/site-packages (from flask->-r requirements.txt (line 2)) (3.1.6)\n",
      "Requirement already satisfied: itsdangerous>=2.2 in /opt/anaconda3/lib/python3.13/site-packages (from flask->-r requirements.txt (line 2)) (2.2.0)\n",
      "Requirement already satisfied: click>=8.1.3 in /opt/anaconda3/lib/python3.13/site-packages (from flask->-r requirements.txt (line 2)) (8.1.8)\n",
      "Requirement already satisfied: blinker>=1.9 in /opt/anaconda3/lib/python3.13/site-packages (from flask->-r requirements.txt (line 2)) (1.9.0)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /opt/anaconda3/lib/python3.13/site-packages (from langchain->-r requirements.txt (line 3)) (6.0.2)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/anaconda3/lib/python3.13/site-packages (from langchain->-r requirements.txt (line 3)) (2.0.35)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /opt/anaconda3/lib/python3.13/site-packages (from langchain->-r requirements.txt (line 3)) (3.11.10)\n",
      "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.0 in /opt/anaconda3/lib/python3.13/site-packages (from langchain->-r requirements.txt (line 3)) (0.3.8)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /opt/anaconda3/lib/python3.13/site-packages (from langchain->-r requirements.txt (line 3)) (0.1.147)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.26.0 in /opt/anaconda3/lib/python3.13/site-packages (from langchain->-r requirements.txt (line 3)) (1.26.4)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /opt/anaconda3/lib/python3.13/site-packages (from langchain->-r requirements.txt (line 3)) (2.12.5)\n",
      "Requirement already satisfied: requests<3,>=2 in /opt/anaconda3/lib/python3.13/site-packages (from langchain->-r requirements.txt (line 3)) (2.32.5)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /opt/anaconda3/lib/python3.13/site-packages (from langchain->-r requirements.txt (line 3)) (9.0.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/anaconda3/lib/python3.13/site-packages (from langchain-core->-r requirements.txt (line 5)) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /opt/anaconda3/lib/python3.13/site-packages (from langchain-core->-r requirements.txt (line 5)) (24.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /opt/anaconda3/lib/python3.13/site-packages (from langchain-core->-r requirements.txt (line 5)) (4.15.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /opt/anaconda3/lib/python3.13/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain->-r requirements.txt (line 3)) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/anaconda3/lib/python3.13/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain->-r requirements.txt (line 3)) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/anaconda3/lib/python3.13/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain->-r requirements.txt (line 3)) (24.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/anaconda3/lib/python3.13/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain->-r requirements.txt (line 3)) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/anaconda3/lib/python3.13/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain->-r requirements.txt (line 3)) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/anaconda3/lib/python3.13/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain->-r requirements.txt (line 3)) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /opt/anaconda3/lib/python3.13/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain->-r requirements.txt (line 3)) (1.18.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /opt/anaconda3/lib/python3.13/site-packages (from jsonpatch<2.0,>=1.33->langchain-core->-r requirements.txt (line 5)) (2.1)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /opt/anaconda3/lib/python3.13/site-packages (from langsmith<0.2.0,>=0.1.17->langchain->-r requirements.txt (line 3)) (0.28.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /opt/anaconda3/lib/python3.13/site-packages (from langsmith<0.2.0,>=0.1.17->langchain->-r requirements.txt (line 3)) (3.11.4)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /opt/anaconda3/lib/python3.13/site-packages (from langsmith<0.2.0,>=0.1.17->langchain->-r requirements.txt (line 3)) (1.0.0)\n",
      "Requirement already satisfied: anyio in /opt/anaconda3/lib/python3.13/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain->-r requirements.txt (line 3)) (4.7.0)\n",
      "Requirement already satisfied: certifi in /opt/anaconda3/lib/python3.13/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain->-r requirements.txt (line 3)) (2025.11.12)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/anaconda3/lib/python3.13/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain->-r requirements.txt (line 3)) (1.0.9)\n",
      "Requirement already satisfied: idna in /opt/anaconda3/lib/python3.13/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain->-r requirements.txt (line 3)) (3.7)\n",
      "Requirement already satisfied: h11>=0.16 in /opt/anaconda3/lib/python3.13/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain->-r requirements.txt (line 3)) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/anaconda3/lib/python3.13/site-packages (from pydantic<3.0.0,>=2.7.4->langchain->-r requirements.txt (line 3)) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in /opt/anaconda3/lib/python3.13/site-packages (from pydantic<3.0.0,>=2.7.4->langchain->-r requirements.txt (line 3)) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /opt/anaconda3/lib/python3.13/site-packages (from pydantic<3.0.0,>=2.7.4->langchain->-r requirements.txt (line 3)) (0.4.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/anaconda3/lib/python3.13/site-packages (from requests<3,>=2->langchain->-r requirements.txt (line 3)) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.13/site-packages (from requests<3,>=2->langchain->-r requirements.txt (line 3)) (2.3.0)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /opt/anaconda3/lib/python3.13/site-packages (from langchain-community->-r requirements.txt (line 4)) (0.6.7)\n",
      "Requirement already satisfied: httpx-sse<0.5.0,>=0.4.0 in /opt/anaconda3/lib/python3.13/site-packages (from langchain-community->-r requirements.txt (line 4)) (0.4.3)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /opt/anaconda3/lib/python3.13/site-packages (from langchain-community->-r requirements.txt (line 4)) (2.12.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /opt/anaconda3/lib/python3.13/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community->-r requirements.txt (line 4)) (3.26.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /opt/anaconda3/lib/python3.13/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community->-r requirements.txt (line 4)) (0.9.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /opt/anaconda3/lib/python3.13/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community->-r requirements.txt (line 4)) (1.0.0)\n",
      "Requirement already satisfied: openai<2.0.0,>=1.68.2 in /opt/anaconda3/lib/python3.13/site-packages (from langchain-openai->-r requirements.txt (line 6)) (1.109.1)\n",
      "Requirement already satisfied: tiktoken<1,>=0.7 in /opt/anaconda3/lib/python3.13/site-packages (from langchain-openai->-r requirements.txt (line 6)) (0.12.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /opt/anaconda3/lib/python3.13/site-packages (from openai<2.0.0,>=1.68.2->langchain-openai->-r requirements.txt (line 6)) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /opt/anaconda3/lib/python3.13/site-packages (from openai<2.0.0,>=1.68.2->langchain-openai->-r requirements.txt (line 6)) (0.10.0)\n",
      "Requirement already satisfied: sniffio in /opt/anaconda3/lib/python3.13/site-packages (from openai<2.0.0,>=1.68.2->langchain-openai->-r requirements.txt (line 6)) (1.3.0)\n",
      "Requirement already satisfied: tqdm>4 in /opt/anaconda3/lib/python3.13/site-packages (from openai<2.0.0,>=1.68.2->langchain-openai->-r requirements.txt (line 6)) (4.67.1)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /opt/anaconda3/lib/python3.13/site-packages (from tiktoken<1,>=0.7->langchain-openai->-r requirements.txt (line 6)) (2024.11.6)\n",
      "Requirement already satisfied: build>=1.0.3 in /opt/anaconda3/lib/python3.13/site-packages (from chromadb->-r requirements.txt (line 11)) (1.3.0)\n",
      "Requirement already satisfied: chroma-hnswlib==0.7.6 in /opt/anaconda3/lib/python3.13/site-packages (from chromadb->-r requirements.txt (line 11)) (0.7.6)\n",
      "Requirement already satisfied: fastapi>=0.95.2 in /opt/anaconda3/lib/python3.13/site-packages (from chromadb->-r requirements.txt (line 11)) (0.124.0)\n",
      "Requirement already satisfied: uvicorn>=0.18.3 in /opt/anaconda3/lib/python3.13/site-packages (from uvicorn[standard]>=0.18.3->chromadb->-r requirements.txt (line 11)) (0.38.0)\n",
      "Requirement already satisfied: posthog>=2.4.0 in /opt/anaconda3/lib/python3.13/site-packages (from chromadb->-r requirements.txt (line 11)) (5.4.0)\n",
      "Requirement already satisfied: onnxruntime>=1.14.1 in /opt/anaconda3/lib/python3.13/site-packages (from chromadb->-r requirements.txt (line 11)) (1.23.2)\n",
      "Requirement already satisfied: opentelemetry-api>=1.2.0 in /opt/anaconda3/lib/python3.13/site-packages (from chromadb->-r requirements.txt (line 11)) (1.39.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in /opt/anaconda3/lib/python3.13/site-packages (from chromadb->-r requirements.txt (line 11)) (1.39.0)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-fastapi>=0.41b0 in /opt/anaconda3/lib/python3.13/site-packages (from chromadb->-r requirements.txt (line 11)) (0.60b0)\n",
      "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /opt/anaconda3/lib/python3.13/site-packages (from chromadb->-r requirements.txt (line 11)) (1.39.0)\n",
      "Requirement already satisfied: tokenizers>=0.13.2 in /opt/anaconda3/lib/python3.13/site-packages (from chromadb->-r requirements.txt (line 11)) (0.22.1)\n",
      "Requirement already satisfied: pypika>=0.48.9 in /opt/anaconda3/lib/python3.13/site-packages (from chromadb->-r requirements.txt (line 11)) (0.48.9)\n",
      "Requirement already satisfied: overrides>=7.3.1 in /opt/anaconda3/lib/python3.13/site-packages (from chromadb->-r requirements.txt (line 11)) (7.4.0)\n",
      "Requirement already satisfied: importlib-resources in /opt/anaconda3/lib/python3.13/site-packages (from chromadb->-r requirements.txt (line 11)) (6.5.2)\n",
      "Requirement already satisfied: grpcio>=1.58.0 in /opt/anaconda3/lib/python3.13/site-packages (from chromadb->-r requirements.txt (line 11)) (1.76.0)\n",
      "Requirement already satisfied: bcrypt>=4.0.1 in /opt/anaconda3/lib/python3.13/site-packages (from chromadb->-r requirements.txt (line 11)) (4.3.0)\n",
      "Requirement already satisfied: typer>=0.9.0 in /opt/anaconda3/lib/python3.13/site-packages (from chromadb->-r requirements.txt (line 11)) (0.9.0)\n",
      "Requirement already satisfied: kubernetes>=28.1.0 in /opt/anaconda3/lib/python3.13/site-packages (from chromadb->-r requirements.txt (line 11)) (34.1.0)\n",
      "Requirement already satisfied: mmh3>=4.0.1 in /opt/anaconda3/lib/python3.13/site-packages (from chromadb->-r requirements.txt (line 11)) (5.2.0)\n",
      "Requirement already satisfied: rich>=10.11.0 in /opt/anaconda3/lib/python3.13/site-packages (from chromadb->-r requirements.txt (line 11)) (13.9.4)\n",
      "Requirement already satisfied: huggingface-hub<2.0,>=0.16.4 in /opt/anaconda3/lib/python3.13/site-packages (from tokenizers>=0.13.2->chromadb->-r requirements.txt (line 11)) (0.36.0)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.13/site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers>=0.13.2->chromadb->-r requirements.txt (line 11)) (3.17.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/anaconda3/lib/python3.13/site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers>=0.13.2->chromadb->-r requirements.txt (line 11)) (2025.3.2)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /opt/anaconda3/lib/python3.13/site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers>=0.13.2->chromadb->-r requirements.txt (line 11)) (1.2.0)\n",
      "Requirement already satisfied: pinecone<8.0.0,>=6.0.0 in /opt/anaconda3/lib/python3.13/site-packages (from pinecone[asyncio]<8.0.0,>=6.0.0->langchain-pinecone->-r requirements.txt (line 9)) (7.3.0)\n",
      "Requirement already satisfied: simsimd>=5.9.11 in /opt/anaconda3/lib/python3.13/site-packages (from langchain-pinecone->-r requirements.txt (line 9)) (6.5.3)\n",
      "Requirement already satisfied: pinecone-plugin-assistant<2.0.0,>=1.6.0 in /opt/anaconda3/lib/python3.13/site-packages (from pinecone<8.0.0,>=6.0.0->pinecone[asyncio]<8.0.0,>=6.0.0->langchain-pinecone->-r requirements.txt (line 9)) (1.8.0)\n",
      "Requirement already satisfied: pinecone-plugin-interface<0.0.8,>=0.0.7 in /opt/anaconda3/lib/python3.13/site-packages (from pinecone<8.0.0,>=6.0.0->pinecone[asyncio]<8.0.0,>=6.0.0->langchain-pinecone->-r requirements.txt (line 9)) (0.0.7)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in /opt/anaconda3/lib/python3.13/site-packages (from pinecone<8.0.0,>=6.0.0->pinecone[asyncio]<8.0.0,>=6.0.0->langchain-pinecone->-r requirements.txt (line 9)) (2.9.0.post0)\n",
      "Requirement already satisfied: aiohttp-retry<3.0.0,>=2.9.1 in /opt/anaconda3/lib/python3.13/site-packages (from pinecone[asyncio]<8.0.0,>=6.0.0->langchain-pinecone->-r requirements.txt (line 9)) (2.9.1)\n",
      "Requirement already satisfied: ollama<1.0.0,>=0.4.8 in /opt/anaconda3/lib/python3.13/site-packages (from langchain-ollama->-r requirements.txt (line 10)) (0.6.1)\n",
      "Requirement already satisfied: filetype in /opt/anaconda3/lib/python3.13/site-packages (from unstructured->-r requirements.txt (line 16)) (1.2.0)\n",
      "Requirement already satisfied: python-magic in /opt/anaconda3/lib/python3.13/site-packages (from unstructured->-r requirements.txt (line 16)) (0.4.27)\n",
      "Requirement already satisfied: lxml in /opt/anaconda3/lib/python3.13/site-packages (from unstructured->-r requirements.txt (line 16)) (5.3.0)\n",
      "Requirement already satisfied: nltk in /opt/anaconda3/lib/python3.13/site-packages (from unstructured->-r requirements.txt (line 16)) (3.9.1)\n",
      "Requirement already satisfied: beautifulsoup4 in /opt/anaconda3/lib/python3.13/site-packages (from unstructured->-r requirements.txt (line 16)) (4.12.3)\n",
      "Requirement already satisfied: emoji in /opt/anaconda3/lib/python3.13/site-packages (from unstructured->-r requirements.txt (line 16)) (2.15.0)\n",
      "Requirement already satisfied: python-iso639 in /opt/anaconda3/lib/python3.13/site-packages (from unstructured->-r requirements.txt (line 16)) (2025.11.16)\n",
      "Requirement already satisfied: langdetect in /opt/anaconda3/lib/python3.13/site-packages (from unstructured->-r requirements.txt (line 16)) (1.0.9)\n",
      "Requirement already satisfied: rapidfuzz in /opt/anaconda3/lib/python3.13/site-packages (from unstructured->-r requirements.txt (line 16)) (3.14.3)\n",
      "Requirement already satisfied: backoff in /opt/anaconda3/lib/python3.13/site-packages (from unstructured->-r requirements.txt (line 16)) (2.2.1)\n",
      "Requirement already satisfied: unstructured-client in /opt/anaconda3/lib/python3.13/site-packages (from unstructured->-r requirements.txt (line 16)) (0.42.4)\n",
      "Requirement already satisfied: wrapt in /opt/anaconda3/lib/python3.13/site-packages (from unstructured->-r requirements.txt (line 16)) (1.17.0)\n",
      "Requirement already satisfied: psutil in /opt/anaconda3/lib/python3.13/site-packages (from unstructured->-r requirements.txt (line 16)) (5.9.0)\n",
      "Requirement already satisfied: python-oxmsg in /opt/anaconda3/lib/python3.13/site-packages (from unstructured->-r requirements.txt (line 16)) (0.0.2)\n",
      "Requirement already satisfied: html5lib in /opt/anaconda3/lib/python3.13/site-packages (from unstructured->-r requirements.txt (line 16)) (1.1)\n",
      "Requirement already satisfied: Pillow>=3.3.2 in /opt/anaconda3/lib/python3.13/site-packages (from python-pptx->-r requirements.txt (line 17)) (11.1.0)\n",
      "Requirement already satisfied: XlsxWriter>=0.5.7 in /opt/anaconda3/lib/python3.13/site-packages (from python-pptx->-r requirements.txt (line 17)) (3.2.9)\n",
      "Requirement already satisfied: langgraph-checkpoint<3.0.0,>=2.1.0 in /opt/anaconda3/lib/python3.13/site-packages (from langgraph->-r requirements.txt (line 18)) (2.1.2)\n",
      "Requirement already satisfied: langgraph-prebuilt<0.6.0,>=0.5.0 in /opt/anaconda3/lib/python3.13/site-packages (from langgraph->-r requirements.txt (line 18)) (0.5.1)\n",
      "Requirement already satisfied: langgraph-sdk<0.2.0,>=0.1.42 in /opt/anaconda3/lib/python3.13/site-packages (from langgraph->-r requirements.txt (line 18)) (0.1.74)\n",
      "Requirement already satisfied: xxhash>=3.5.0 in /opt/anaconda3/lib/python3.13/site-packages (from langgraph->-r requirements.txt (line 18)) (3.6.0)\n",
      "Requirement already satisfied: ormsgpack>=1.10.0 in /opt/anaconda3/lib/python3.13/site-packages (from langgraph-checkpoint<3.0.0,>=2.1.0->langgraph->-r requirements.txt (line 18)) (1.12.0)\n",
      "Requirement already satisfied: pyproject_hooks in /opt/anaconda3/lib/python3.13/site-packages (from build>=1.0.3->chromadb->-r requirements.txt (line 11)) (1.2.0)\n",
      "Requirement already satisfied: starlette<0.51.0,>=0.40.0 in /opt/anaconda3/lib/python3.13/site-packages (from fastapi>=0.95.2->chromadb->-r requirements.txt (line 11)) (0.50.0)\n",
      "Requirement already satisfied: annotated-doc>=0.0.2 in /opt/anaconda3/lib/python3.13/site-packages (from fastapi>=0.95.2->chromadb->-r requirements.txt (line 11)) (0.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/lib/python3.13/site-packages (from Jinja2>=3.1.2->flask->-r requirements.txt (line 2)) (3.0.2)\n",
      "Requirement already satisfied: six>=1.9.0 in /opt/anaconda3/lib/python3.13/site-packages (from kubernetes>=28.1.0->chromadb->-r requirements.txt (line 11)) (1.17.0)\n",
      "Requirement already satisfied: google-auth>=1.0.1 in /opt/anaconda3/lib/python3.13/site-packages (from kubernetes>=28.1.0->chromadb->-r requirements.txt (line 11)) (2.43.0)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /opt/anaconda3/lib/python3.13/site-packages (from kubernetes>=28.1.0->chromadb->-r requirements.txt (line 11)) (1.8.0)\n",
      "Requirement already satisfied: requests-oauthlib in /opt/anaconda3/lib/python3.13/site-packages (from kubernetes>=28.1.0->chromadb->-r requirements.txt (line 11)) (2.0.0)\n",
      "Requirement already satisfied: durationpy>=0.7 in /opt/anaconda3/lib/python3.13/site-packages (from kubernetes>=28.1.0->chromadb->-r requirements.txt (line 11)) (0.10)\n",
      "Requirement already satisfied: cachetools<7.0,>=2.0.0 in /opt/anaconda3/lib/python3.13/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb->-r requirements.txt (line 11)) (5.5.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/anaconda3/lib/python3.13/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb->-r requirements.txt (line 11)) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/anaconda3/lib/python3.13/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb->-r requirements.txt (line 11)) (4.9.1)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in /opt/anaconda3/lib/python3.13/site-packages (from rsa<5,>=3.1.4->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb->-r requirements.txt (line 11)) (0.4.8)\n",
      "Requirement already satisfied: coloredlogs in /opt/anaconda3/lib/python3.13/site-packages (from onnxruntime>=1.14.1->chromadb->-r requirements.txt (line 11)) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in /opt/anaconda3/lib/python3.13/site-packages (from onnxruntime>=1.14.1->chromadb->-r requirements.txt (line 11)) (25.9.23)\n",
      "Requirement already satisfied: protobuf in /opt/anaconda3/lib/python3.13/site-packages (from onnxruntime>=1.14.1->chromadb->-r requirements.txt (line 11)) (5.29.3)\n",
      "Requirement already satisfied: sympy in /opt/anaconda3/lib/python3.13/site-packages (from onnxruntime>=1.14.1->chromadb->-r requirements.txt (line 11)) (1.13.3)\n",
      "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in /opt/anaconda3/lib/python3.13/site-packages (from opentelemetry-api>=1.2.0->chromadb->-r requirements.txt (line 11)) (8.5.0)\n",
      "Requirement already satisfied: zipp>=3.20 in /opt/anaconda3/lib/python3.13/site-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api>=1.2.0->chromadb->-r requirements.txt (line 11)) (3.21.0)\n",
      "Requirement already satisfied: googleapis-common-protos~=1.57 in /opt/anaconda3/lib/python3.13/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb->-r requirements.txt (line 11)) (1.72.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.39.0 in /opt/anaconda3/lib/python3.13/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb->-r requirements.txt (line 11)) (1.39.0)\n",
      "Requirement already satisfied: opentelemetry-proto==1.39.0 in /opt/anaconda3/lib/python3.13/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb->-r requirements.txt (line 11)) (1.39.0)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.60b0 in /opt/anaconda3/lib/python3.13/site-packages (from opentelemetry-sdk>=1.2.0->chromadb->-r requirements.txt (line 11)) (0.60b0)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-asgi==0.60b0 in /opt/anaconda3/lib/python3.13/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb->-r requirements.txt (line 11)) (0.60b0)\n",
      "Requirement already satisfied: opentelemetry-instrumentation==0.60b0 in /opt/anaconda3/lib/python3.13/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb->-r requirements.txt (line 11)) (0.60b0)\n",
      "Requirement already satisfied: opentelemetry-util-http==0.60b0 in /opt/anaconda3/lib/python3.13/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb->-r requirements.txt (line 11)) (0.60b0)\n",
      "Requirement already satisfied: asgiref~=3.0 in /opt/anaconda3/lib/python3.13/site-packages (from opentelemetry-instrumentation-asgi==0.60b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb->-r requirements.txt (line 11)) (3.11.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/anaconda3/lib/python3.13/site-packages (from rich>=10.11.0->chromadb->-r requirements.txt (line 11)) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/anaconda3/lib/python3.13/site-packages (from rich>=10.11.0->chromadb->-r requirements.txt (line 11)) (2.19.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/anaconda3/lib/python3.13/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb->-r requirements.txt (line 11)) (0.1.0)\n",
      "Requirement already satisfied: httptools>=0.6.3 in /opt/anaconda3/lib/python3.13/site-packages (from uvicorn[standard]>=0.18.3->chromadb->-r requirements.txt (line 11)) (0.7.1)\n",
      "Requirement already satisfied: uvloop>=0.15.1 in /opt/anaconda3/lib/python3.13/site-packages (from uvicorn[standard]>=0.18.3->chromadb->-r requirements.txt (line 11)) (0.22.1)\n",
      "Requirement already satisfied: watchfiles>=0.13 in /opt/anaconda3/lib/python3.13/site-packages (from uvicorn[standard]>=0.18.3->chromadb->-r requirements.txt (line 11)) (1.1.1)\n",
      "Requirement already satisfied: websockets>=10.4 in /opt/anaconda3/lib/python3.13/site-packages (from uvicorn[standard]>=0.18.3->chromadb->-r requirements.txt (line 11)) (15.0.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in /opt/anaconda3/lib/python3.13/site-packages (from beautifulsoup4->unstructured->-r requirements.txt (line 16)) (2.5)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /opt/anaconda3/lib/python3.13/site-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb->-r requirements.txt (line 11)) (10.0)\n",
      "Requirement already satisfied: webencodings in /opt/anaconda3/lib/python3.13/site-packages (from html5lib->unstructured->-r requirements.txt (line 16)) (0.5.1)\n",
      "Requirement already satisfied: joblib in /opt/anaconda3/lib/python3.13/site-packages (from nltk->unstructured->-r requirements.txt (line 16)) (1.4.2)\n",
      "Requirement already satisfied: olefile in /opt/anaconda3/lib/python3.13/site-packages (from python-oxmsg->unstructured->-r requirements.txt (line 16)) (0.47)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/anaconda3/lib/python3.13/site-packages (from requests-oauthlib->kubernetes>=28.1.0->chromadb->-r requirements.txt (line 11)) (3.3.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/lib/python3.13/site-packages (from sympy->onnxruntime>=1.14.1->chromadb->-r requirements.txt (line 11)) (1.3.0)\n",
      "Requirement already satisfied: aiofiles>=24.1.0 in /opt/anaconda3/lib/python3.13/site-packages (from unstructured-client->unstructured->-r requirements.txt (line 16)) (25.1.0)\n",
      "Requirement already satisfied: cryptography>=3.1 in /opt/anaconda3/lib/python3.13/site-packages (from unstructured-client->unstructured->-r requirements.txt (line 16)) (44.0.1)\n",
      "Requirement already satisfied: cffi>=1.12 in /opt/anaconda3/lib/python3.13/site-packages (from cryptography>=3.1->unstructured-client->unstructured->-r requirements.txt (line 16)) (1.17.1)\n",
      "Requirement already satisfied: pycparser in /opt/anaconda3/lib/python3.13/site-packages (from cffi>=1.12->cryptography>=3.1->unstructured-client->unstructured->-r requirements.txt (line 16)) (2.21)\n"
     ]
    }
   ],
   "source": [
    "# Install project requirements\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38bdd461-7dbf-44c4-b970-9335a9173224",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created .env file successfully. Ensure that Ollama is installed locally.\n"
     ]
    }
   ],
   "source": [
    "env_content = \"\"\"\n",
    "# ======================================\n",
    "# LLM Provider Settings\n",
    "# ======================================\n",
    "LLM_PROVIDER=ollama\n",
    "# Default local model (must be installed using: ollama pull llama3)\n",
    "OLLAMA_MODEL=llama3\n",
    "\n",
    "# ======================================\n",
    "# Embedding Model Settings\n",
    "# ======================================\n",
    "# Local embeddings using Ollama\n",
    "EMBED_PROVIDER=ollama\n",
    "EMBED_MODEL=nomic-embed-text\n",
    "\n",
    "# ======================================\n",
    "# Vector Store Settings\n",
    "# ======================================\n",
    "VECTOR_DB=chroma\n",
    "COLLECTION=docval\n",
    "\n",
    "# ======================================\n",
    "# API Keys (Optional)\n",
    "# ======================================\n",
    "# Leave these empty if using local models\n",
    "OPENAI_API_KEY=\n",
    "MISTRAL_API_KEY=\n",
    "\n",
    "# Optional: Pinecone (not used by default)\n",
    "PINECONE_API_KEY=\n",
    "PINECONE_INDEX=docval\n",
    "\"\"\"\n",
    "\n",
    "with open(\".env\", \"w\") as f:\n",
    "    f.write(env_content)\n",
    "\n",
    "print(\"Created .env file successfully. Ensure that Ollama is installed locally.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c0bfd6-baf9-4541-8982-f23fad9bc270",
   "metadata": {},
   "source": [
    "## 1. Imports, Paths, and Environment Setup\n",
    "\n",
    "This notebook begins by importing the core libraries used for the RAG (Retrieval-Augmented Generation) pipeline and setting up the project directory structure.\n",
    "\n",
    "### Standard Python utilities\n",
    "- `os` and `json` for file and path operations.\n",
    "- `dotenv.load_dotenv` for loading configuration values (model names, provider settings) from the `.env` file.\n",
    "\n",
    "### LangChain document loaders\n",
    "These classes allow reading different document formats into a unified `Document` structure:\n",
    "- `PyPDFLoader` for PDF files  \n",
    "- `Docx2txtLoader` for DOCX files  \n",
    "- (Optional) `UnstructuredPowerPointLoader` for PPTX files  \n",
    "\n",
    "### Text splitting\n",
    "- `RecursiveCharacterTextSplitter` is used to divide long documents into semantically coherent chunks suitable for embedding.\n",
    "\n",
    "### Embeddings and vector store\n",
    "The framework supports:\n",
    "- **Ollama embeddings** (default, fully local and free)\n",
    "- **OpenAI embeddings** (optional, if API key is provided)\n",
    "\n",
    "For retrieval, the system uses:\n",
    "- **Chroma**, a persistent vector database stored locally.\n",
    "\n",
    "### Language models\n",
    "The notebook loads a chat model via a helper function that can switch between:\n",
    "- Ollama (default, local)\n",
    "- OpenAI (optional)\n",
    "- Mistral (optional)\n",
    "\n",
    "### Directory setup\n",
    "- `BASE_DIR` points to the project root.\n",
    "- `DATA_DIR` contains the documents to ingest.\n",
    "- `VECTOR_DIR` stores the vector database.\n",
    "\n",
    "### Environment configuration\n",
    "`load_dotenv()` loads all settings from `.env`, allowing model providers and embedding options to be selected without modifying code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae6a8813-4710-4b0a-b644-7520489456c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment loaded.\n",
      "BASE_DIR: /Users/mona/test project of rag\n",
      "DATA_DIR: /Users/mona/test project of rag/data\n",
      "VECTOR_DIR: /Users/mona/test project of rag/vector\n"
     ]
    }
   ],
   "source": [
    "# Core Python utilities\n",
    "import os\n",
    "import json\n",
    "\n",
    "# Environment variable loader\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# LangChain document loaders\n",
    "from langchain_community.document_loaders import (\n",
    "    Docx2txtLoader,\n",
    "    PyPDFLoader\n",
    ")\n",
    "\n",
    "# Optional PPTX support (enable only if needed)\n",
    "# from langchain_community.document_loaders import UnstructuredPowerPointLoader\n",
    "\n",
    "# Text splitting\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Document container\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "# Load environment variables from .env\n",
    "load_dotenv()\n",
    "\n",
    "# Project directories\n",
    "BASE_DIR = os.getcwd()\n",
    "DATA_DIR = os.path.join(BASE_DIR, \"data\")\n",
    "VECTOR_DIR = os.path.join(BASE_DIR, \"vector\")\n",
    "\n",
    "print(\"Environment loaded.\")\n",
    "print(\"BASE_DIR:\", BASE_DIR)\n",
    "print(\"DATA_DIR:\", DATA_DIR)\n",
    "print(\"VECTOR_DIR:\", VECTOR_DIR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a17b38c8-c906-4c9c-a4a9-7bf2f5f9af49",
   "metadata": {},
   "source": [
    "## 2. LLM Selection (`get_chat_model`)\n",
    "\n",
    "This helper function selects the large language model (LLM) backend based on\n",
    "settings stored in the `.env` file. The rest of the notebook simply calls\n",
    "`get_chat_model()` without worrying about which provider is used underneath.\n",
    "\n",
    "### Supported providers\n",
    "- **Ollama** (`LLM_PROVIDER=ollama`)  \n",
    "  Default choice. Runs fully locally and does not require API keys.\n",
    "\n",
    "- **OpenAI** (`LLM_PROVIDER=openai`)  \n",
    "  Optional. Used only if an API key is provided.\n",
    "\n",
    "- **Mistral** (`LLM_PROVIDER=mistral`)  \n",
    "  Optional. Requires an API key.\n",
    "\n",
    "### Environment variables controlling the behavior\n",
    "- `LLM_PROVIDER`  \n",
    "  Determines which backend to use: `ollama`, `openai`, or `mistral`.\n",
    "\n",
    "- `OLLAMA_MODEL`  \n",
    "  Model name for Ollama (e.g. `llama3.1`).\n",
    "\n",
    "- `CHAT_MODEL`  \n",
    "  OpenAI model name (e.g. `gpt-4o-mini`). Only used if provider is OpenAI.\n",
    "\n",
    "- `MISTRAL_MODEL`  \n",
    "  Model name for the Mistral API.\n",
    "\n",
    "The goal is to keep the RAG pipeline provider-agnostic: the same codebase can\n",
    "run entirely locally (Ollama) or switch to hosted LLM services simply by\n",
    "changing `.env` variables.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0a0faad7-3e40-4686-a2cc-469888b19ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_mistralai import ChatMistralAI\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "import os\n",
    "\n",
    "def get_chat_model():\n",
    "    \"\"\"\n",
    "    Selects the chat model backend based on environment variables.\n",
    "    Supports: Ollama (default), OpenAI, Mistral.\n",
    "    \"\"\"\n",
    "    provider = os.getenv(\"LLM_PROVIDER\", \"ollama\").lower()\n",
    "\n",
    "    # -------- OpenAI backend --------\n",
    "    if provider == \"openai\":\n",
    "        api_key = os.getenv(\"OPENAI_API_KEY\", \"\").strip()\n",
    "        if not api_key:\n",
    "            raise ValueError(\"OPENAI_API_KEY is not set. Provide a key or switch provider to 'ollama'.\")\n",
    "        model = os.getenv(\"CHAT_MODEL\", \"gpt-4o-mini\")\n",
    "        return ChatOpenAI(model=model, temperature=0)\n",
    "\n",
    "    # -------- Mistral backend --------\n",
    "    if provider == \"mistral\":\n",
    "        api_key = os.getenv(\"MISTRAL_API_KEY\", \"\").strip()\n",
    "        if not api_key:\n",
    "            raise ValueError(\"MISTRAL_API_KEY is not set. Provide a key or switch provider to 'ollama'.\")\n",
    "        model = os.getenv(\"MISTRAL_MODEL\", \"mistral-large-latest\")\n",
    "        return ChatMistralAI(model=model, temperature=0)\n",
    "\n",
    "    # -------- Ollama backend (default, local, free) --------\n",
    "    if provider == \"ollama\":\n",
    "        model = os.getenv(\"OLLAMA_MODEL\", \"llama3.1\")\n",
    "        return ChatOllama(model=model, temperature=0)\n",
    "\n",
    "    # -------- Fallback --------\n",
    "    return ChatOllama(model=\"llama3.1\", temperature=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3cf91f6d-3884-47f3-8e5e-ee42ef4670b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/36/5wb7gt6s3h92jhx7fj1lm3hm0000gn/T/ipykernel_47656/3468636113.py:32: LangChainDeprecationWarning: The class `ChatOllama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import ChatOllama``.\n",
      "  return ChatOllama(model=model, temperature=0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ChatOllama(model='llama3', temperature=0.0)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm = get_chat_model()\n",
    "llm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fff68ca-48d7-47cd-aab8-e614fb7e5e06",
   "metadata": {},
   "source": [
    "## 3. Vector Store Selection (`get_retriever`)\n",
    "\n",
    "This function selects the vector database backend used for semantic retrieval.\n",
    "In this notebook, the default and fully implemented backend is **Chroma**, a\n",
    "local on-disk persistent vector store.\n",
    "\n",
    "The retriever returned by this function is used to:\n",
    "1. Encode incoming queries into embeddings.\n",
    "2. Perform similarity search or Max Marginal Relevance (MMR) search.\n",
    "3. Return the most relevant document chunks to the RAG pipeline.\n",
    "\n",
    "### Supported backend in this implementation\n",
    "- **Chroma** (`VECTOR_DB=chroma`)  \n",
    "  Persistent local vector store. Fast, reliable, and ideal for a lightweight\n",
    "  document validation framework.\n",
    "\n",
    "### Optional / not enabled by default\n",
    "The framework is structured so that FAISS and Pinecone could be added, but they\n",
    "are not active in this notebook:\n",
    "- `faiss` â€“ in-memory local index  \n",
    "- `pinecone` â€“ managed cloud vector database  \n",
    "\n",
    "Only Chroma is used for ingestion, retrieval, and validation.\n",
    "\n",
    "### Behavior\n",
    "`get_retriever()`:\n",
    "1. Loads the embedding model according to `.env` settings  \n",
    "   (Ollama embeddings by default, OpenAI if configured).  \n",
    "2. Opens the Chroma collection stored in `VECTOR_DIR`.  \n",
    "3. Wraps it as a retriever that supports `.invoke(query)` to obtain relevant\n",
    "   chunks.\n",
    "\n",
    "This design allows the rest of the system to remain backend-agnostic.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5dd3b115-9d9f-4208-9e68-299a2d3d95ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embeddings\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "\n",
    "# Vector store (Chroma only, since this notebook uses Chroma for retrieval)\n",
    "from langchain_chroma import Chroma\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "07ca3f16-a526-47a1-a2bc-d82bab4a4f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "import os\n",
    "\n",
    "def get_embeddings():\n",
    "    \"\"\"\n",
    "    Select the embedding model backend.\n",
    "    Default: local Ollama embeddings (free).\n",
    "    Optional: OpenAI embeddings if configured in .env.\n",
    "    \"\"\"\n",
    "    provider = os.getenv(\"EMBED_PROVIDER\", \"ollama\").lower()\n",
    "    model = os.getenv(\"EMBED_MODEL\", \"nomic-embed-text\")\n",
    "\n",
    "    print(f\"[Embedding] Provider: {provider}\")\n",
    "    print(f\"[Embedding] Model: {model}\")\n",
    "\n",
    "    # Local free embeddings\n",
    "    if provider == \"ollama\":\n",
    "        return OllamaEmbeddings(model=model)\n",
    "\n",
    "    # Optional: OpenAI embeddings\n",
    "    from langchain_openai import OpenAIEmbeddings\n",
    "    return OpenAIEmbeddings(model=model)\n",
    "\n",
    "\n",
    "def get_retriever():\n",
    "    \"\"\"\n",
    "    Initialize the vector database (Chroma) and return a retriever.\n",
    "    This implementation focuses on Chroma as the persistent local store.\n",
    "    \"\"\"\n",
    "    print(\"======================================\")\n",
    "    print(\"      INITIALIZING RETRIEVER\")\n",
    "    print(\"======================================\")\n",
    "\n",
    "    embeddings = get_embeddings()  # also prints debug info\n",
    "\n",
    "    persist_dir = os.path.join(VECTOR_DIR, \"chroma\")\n",
    "    os.makedirs(persist_dir, exist_ok=True)\n",
    "\n",
    "    print(f\"[Chroma] Persist directory: {persist_dir}\")\n",
    "    print(\"[Chroma] Loading or creating Chroma DB...\")\n",
    "\n",
    "    vectordb = Chroma(\n",
    "        collection_name=os.getenv(\"COLLECTION\", \"docval\"),\n",
    "        embedding_function=embeddings,\n",
    "        persist_directory=persist_dir,\n",
    "    )\n",
    "\n",
    "    print(\"[Chroma] Vector DB ready.\")\n",
    "    print(\"[Chroma] Returning MMR retriever.\\n\")\n",
    "\n",
    "    return vectordb.as_retriever(\n",
    "        search_type=\"mmr\",\n",
    "        search_kwargs={\"k\": 8, \"fetch_k\": 32, \"lambda_mult\": 0.5},\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a2851cb1-3af7-41c0-9a69-94f63c4c0733",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- TESTING EMBEDDINGS ----\n",
      "[Embedding] Provider: ollama\n",
      "[Embedding] Model: nomic-embed-text\n",
      "Embedding object: base_url='http://localhost:11434' model='nomic-embed-text' embed_instruction='passage: ' query_instruction='query: ' mirostat=None mirostat_eta=None mirostat_tau=None num_ctx=None num_gpu=None num_thread=None repeat_last_n=None repeat_penalty=None temperature=None stop=None tfs_z=None top_k=None top_p=None show_progress=False headers=None model_kwargs=None\n",
      "\n",
      "---- TESTING RETRIEVER ----\n",
      "======================================\n",
      "      INITIALIZING RETRIEVER\n",
      "======================================\n",
      "[Embedding] Provider: ollama\n",
      "[Embedding] Model: nomic-embed-text\n",
      "[Chroma] Persist directory: /Users/mona/test project of rag/vector/chroma\n",
      "[Chroma] Loading or creating Chroma DB...\n",
      "[Chroma] Vector DB ready.\n",
      "[Chroma] Returning MMR retriever.\n",
      "\n",
      "Retriever object: tags=['Chroma', 'OllamaEmbeddings'] vectorstore=<langchain_chroma.vectorstores.Chroma object at 0x113230440> search_type='mmr' search_kwargs={'k': 8, 'fetch_k': 32, 'lambda_mult': 0.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/36/5wb7gt6s3h92jhx7fj1lm3hm0000gn/T/ipykernel_47656/1764496543.py:19: LangChainDeprecationWarning: The class `OllamaEmbeddings` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaEmbeddings``.\n",
      "  return OllamaEmbeddings(model=model)\n"
     ]
    }
   ],
   "source": [
    "print(\"---- TESTING EMBEDDINGS ----\")\n",
    "emb = get_embeddings()\n",
    "print(\"Embedding object:\", emb)\n",
    "\n",
    "print(\"\\n---- TESTING RETRIEVER ----\")\n",
    "retriever = get_retriever()\n",
    "print(\"Retriever object:\", retriever)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18dfa945-4577-449f-a331-ef9482badba9",
   "metadata": {},
   "source": [
    "## 4. File Loading and Text Chunking Helpers\n",
    "\n",
    "This section defines the utility functions used to load documents from disk and\n",
    "convert them into text chunks suitable for embedding and retrieval.\n",
    "\n",
    "### `_file_type(path)`\n",
    "Identifies the file type from the extension and returns a simple label:\n",
    "- `.docx` â†’ `docx`\n",
    "- `.pdf` â†’ `pdf`\n",
    "- `.pptx` â†’ `pptx`\n",
    "- anything else â†’ `other` (ignored)\n",
    "\n",
    "### `_load_text(abs_path, ftype)`\n",
    "Loads the raw text from a document according to its type:\n",
    "- `docx`: extracted using `Docx2txtLoader`\n",
    "- `pdf`: loaded page-by-page using `PyPDFLoader`, then pages concatenated\n",
    "- `pptx`: (optional) loaded using `UnstructuredPowerPointLoader`\n",
    "- unsupported types are skipped\n",
    "\n",
    "This function always returns a single text string representing the full document.\n",
    "\n",
    "### `_chunk_text(text)`\n",
    "Splits long text into overlapping chunks using `RecursiveCharacterTextSplitter`\n",
    "with:\n",
    "- `chunk_size = 1000` characters  \n",
    "- `chunk_overlap = 200` characters  \n",
    "\n",
    "This produces a list of LangChain `Document` objects, each containing a text\n",
    "chunk and associated metadata.\n",
    "\n",
    "The purpose of these helpers is to standardize preprocessing so that all\n",
    "documents, regardless of file type, follow a consistent pipeline before being\n",
    "embedded and indexed in the vector store.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cc26b553-f9eb-4729-bd33-d1d2f5a792cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ” File helper functions LOADED.\n"
     ]
    }
   ],
   "source": [
    "def _file_type(path: str) -> str:\n",
    "    ext = os.path.splitext(path)[1].lower()\n",
    "    if ext == \".docx\":\n",
    "        return \"docx\"\n",
    "    if ext == \".pdf\":\n",
    "        return \"pdf\"\n",
    "    if ext == \".pptx\":\n",
    "        return \"pptx\"\n",
    "    return \"other\"\n",
    "\n",
    "\n",
    "def _load_text(abs_path: str, ftype: str) -> str:\n",
    "    if ftype == \"docx\":\n",
    "        docs = Docx2txtLoader(abs_path).load()\n",
    "        return docs[0].page_content\n",
    "    if ftype == \"pdf\":\n",
    "        docs = PyPDFLoader(abs_path).load()\n",
    "        return \"\\n\\n\".join(d.page_content for d in docs)\n",
    "    if ftype == \"pptx\":\n",
    "        slides = UnstructuredPowerPointLoader(abs_path).load()\n",
    "        return \"\\n\\n\".join(d.page_content or \"\" for d in slides)\n",
    "    raise ValueError(f\"Unsupported file type: {ftype}\")\n",
    "\n",
    "\n",
    "def _chunk_text(text: str):\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000,\n",
    "        chunk_overlap=200,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \". \", \" \"],\n",
    "    )\n",
    "    return splitter.split_documents([Document(page_content=text)])\n",
    "\n",
    "print(\"âœ” File helper functions LOADED.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba3b4fb7-4a49-45bd-bacb-5966e43e3064",
   "metadata": {},
   "source": [
    "## 5. Ingestion Pipeline (`ingest`)\n",
    "\n",
    "The `ingest()` function processes all documents in the `data/` directory and\n",
    "prepares them for retrieval. It performs three main tasks:\n",
    "\n",
    "1. **Load raw documents**  \n",
    "   The function scans the `data/` folder recursively.  \n",
    "   For each supported file (`.pdf`, `.docx`, optional `.pptx`):\n",
    "   - determine its type using `_file_type()`\n",
    "   - load the full text using `_load_text()`\n",
    "   - split the text into chunks using `_chunk_text()`\n",
    "\n",
    "2. **Add metadata**  \n",
    "   Each chunk is wrapped in a `Document` object with metadata including:\n",
    "   - source file path  \n",
    "   - file name  \n",
    "   - file type  \n",
    "   - ingestion timestamp  \n",
    "\n",
    "3. **Store embeddings in a vector database**  \n",
    "   The system uses **ChromaDB** as the persistent local vector store.  \n",
    "   Steps:\n",
    "   - initialize the embedding model (`get_embeddings()`)\n",
    "   - open or create the Chroma collection in `VECTOR_DIR`\n",
    "   - insert all chunks with their embeddings\n",
    "\n",
    "At the end of the process, the vector database contains semantically searchable\n",
    "representations of all documents. This database is later used by the retriever\n",
    "for RAG-based question answering and requirement validation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b7dbece5-e6f3-4559-8941-4b225fe2b2f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b1b46352-7b52-440b-8b92-9e8101ed6ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "def ingest():\n",
    "    \"\"\"\n",
    "    Load all documents from DATA_DIR, chunk them, and store them in Chroma.\n",
    "    This is the only backend supported in this notebook.\n",
    "    \"\"\"\n",
    "    print(\"======================================\")\n",
    "    print(\"            INGEST START\")\n",
    "    print(\"======================================\")\n",
    "\n",
    "    os.makedirs(DATA_DIR, exist_ok=True)\n",
    "\n",
    "    embeddings = get_embeddings()\n",
    "    all_docs = []\n",
    "\n",
    "    print(f\"[Ingest] Scanning folder: {DATA_DIR}\")\n",
    "\n",
    "    # -----------------------------------------------------\n",
    "    # Load all supported files in DATA_DIR\n",
    "    # -----------------------------------------------------\n",
    "    for root, _, files in os.walk(DATA_DIR):\n",
    "        if \".ipynb_checkpoints\" in root:\n",
    "            continue\n",
    "\n",
    "        for name in files:\n",
    "            if name.startswith(\"~$\"):\n",
    "                continue\n",
    "\n",
    "            ftype = _file_type(name)\n",
    "            if ftype == \"other\":\n",
    "                continue\n",
    "\n",
    "            path = os.path.join(root, name)\n",
    "            print(f\"[Load] {path} (type={ftype})\")\n",
    "\n",
    "            text = _load_text(path, ftype)\n",
    "            if not text.strip():\n",
    "                print(f\"[Skip] Empty file: {name}\")\n",
    "                continue\n",
    "\n",
    "            chunks = _chunk_text(text)\n",
    "            print(f\"[Chunks] {len(chunks)} chunks created.\")\n",
    "\n",
    "            ts = datetime.now(timezone.utc).isoformat()\n",
    "\n",
    "\n",
    "            for chunk in chunks:\n",
    "                all_docs.append(\n",
    "                    Document(\n",
    "                        page_content=chunk.page_content,\n",
    "                        metadata={\n",
    "                            \"source_path\": path.replace(\"\\\\\", \"/\"),\n",
    "                            \"filename\": name,\n",
    "                            \"type\": ftype,\n",
    "                            \"ingested_at\": ts,\n",
    "                        },\n",
    "                    )\n",
    "                )\n",
    "\n",
    "    if not all_docs:\n",
    "        print(\"[Ingest] No supported documents found.\")\n",
    "        print(\"======================================\")\n",
    "        print(\"            INGEST END\")\n",
    "        print(\"======================================\")\n",
    "        return {\"backend\": \"chroma\", \"count\": 0}\n",
    "\n",
    "    print(f\"[Ingest] Total chunks to index: {len(all_docs)}\")\n",
    "\n",
    "    # -----------------------------------------------------\n",
    "    # Store in Chroma\n",
    "    # -----------------------------------------------------\n",
    "    persist_dir = os.path.join(VECTOR_DIR, \"chroma\")\n",
    "    os.makedirs(persist_dir, exist_ok=True)\n",
    "\n",
    "    print(f\"[Chroma] Persist directory: {persist_dir}\")\n",
    "\n",
    "    vectordb = Chroma(\n",
    "        collection_name=os.getenv(\"COLLECTION\", \"docval\"),\n",
    "        embedding_function=embeddings,\n",
    "        persist_directory=persist_dir,\n",
    "    )\n",
    "\n",
    "    vectordb.add_documents(all_docs)\n",
    "    print(\"[Chroma] Documents added to vector store.\")\n",
    "\n",
    "    print(\"======================================\")\n",
    "    print(\"             INGEST END\")\n",
    "    print(\"======================================\")\n",
    "\n",
    "    return {\"backend\": \"chroma\", \"count\": len(all_docs)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "be9e1039-30de-4d7c-9fc9-d0319378175b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================\n",
      "            INGEST START\n",
      "======================================\n",
      "[Embedding] Provider: ollama\n",
      "[Embedding] Model: nomic-embed-text\n",
      "[Ingest] Scanning folder: /Users/mona/test project of rag/data\n",
      "[Load] /Users/mona/test project of rag/data/ACME_Information_Security_Policy.pdf (type=pdf)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ignoring wrong pointing object 6 0 (offset 0)\n",
      "Ignoring wrong pointing object 8 0 (offset 0)\n",
      "Ignoring wrong pointing object 30 0 (offset 0)\n",
      "Ignoring wrong pointing object 31 0 (offset 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Chunks] 12 chunks created.\n",
      "[Ingest] Total chunks to index: 12\n",
      "[Chroma] Persist directory: /Users/mona/test project of rag/vector/chroma\n",
      "[Chroma] Documents added to vector store.\n",
      "======================================\n",
      "             INGEST END\n",
      "======================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'backend': 'chroma', 'count': 12}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = ingest()\n",
    "result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8733dc39-82c6-4982-9703-c0f67a861a30",
   "metadata": {},
   "source": [
    "## 6. RAG Chat Function (`chat`)\n",
    "\n",
    "This section implements the main question-answering function using a\n",
    "Retrieval-Augmented Generation (RAG) pipeline.\n",
    "\n",
    "### `_concat(docs)`\n",
    "Helper function that joins the `page_content` of retrieved documents into a\n",
    "single context string.\n",
    "\n",
    "### `chat(question)`\n",
    "This function executes a full RAG pass:\n",
    "\n",
    "1. Initialize the retriever and language model.\n",
    "2. Retrieve the most relevant document chunks for the question.\n",
    "3. Concatenate the chunks into a contextual text block.\n",
    "4. Build a prompt instructing the model to:\n",
    "   - answer strictly using the provided context  \n",
    "   - admit \"I don't know\" when the context does not contain the answer\n",
    "5. Run the LLM on the question + context.\n",
    "6. Return a dictionary containing:\n",
    "   - `answer`: model output  \n",
    "   - `context`: text actually used for grounding  \n",
    "   - `docs`: list of retrieved chunks and their metadata  \n",
    "\n",
    "This is the core RAG function used throughout the notebook to query the indexed documents.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7df7df1d-9486-4a7e-a548-574671ab68d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "def _concat(docs):\n",
    "    if not docs:\n",
    "        return \"\"\n",
    "    return \"\\n\\n\".join(d.page_content for d in docs)\n",
    "\n",
    "\n",
    "def chat(question: str):\n",
    "    print(\"\\n======================================\")\n",
    "    print(\"               CHAT START\")\n",
    "    print(\"======================================\")\n",
    "\n",
    "    retriever = get_retriever()\n",
    "    llm = get_chat_model()\n",
    "\n",
    "    print(\"[Chat] Retrieving documents...\")\n",
    "    docs = retriever.invoke(question)   # modern LangChain API\n",
    "    print(f\"[Chat] Retrieved {len(docs)} docs.\")\n",
    "\n",
    "    # If nothing retrieved â†’ return safe â€œI donâ€™t knowâ€\n",
    "    if not docs:\n",
    "        print(\"[Chat] No documents retrieved. Returning fallback answer.\")\n",
    "        return {\n",
    "            \"answer\": \"I don't know. No relevant content was found in the indexed documents.\",\n",
    "            \"context\": \"\",\n",
    "            \"docs\": [],\n",
    "        }\n",
    "\n",
    "    context = _concat(docs)\n",
    "\n",
    "    # Build clean prompt\n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\n",
    "                \"system\",\n",
    "                (\n",
    "                    \"You are a strict document assistant. \"\n",
    "                    \"Answer ONLY using the provided context. \"\n",
    "                    \"If the answer is not contained in the context, respond with: I don't know.\"\n",
    "                )\n",
    "            ),\n",
    "            (\"system\", \"Context:\\n{context}\"),\n",
    "            (\"human\", \"{question}\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    chain = prompt | llm\n",
    "\n",
    "    print(\"[Chat] Calling LLM...\")\n",
    "    response = chain.invoke(\n",
    "        {\n",
    "            \"context\": context,\n",
    "            \"question\": question,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    answer_text = response.content\n",
    "    print(\"[Chat] DONE.\")\n",
    "    print(\"======================================\")\n",
    "\n",
    "    return {\n",
    "    \"answer\": answer_text,\n",
    "    \"context\": context,\n",
    "    \"docs\": [\n",
    "        {\n",
    "            \"source_path\": d.metadata.get(\"source_path\"),\n",
    "            \"filename\": d.metadata.get(\"filename\"),\n",
    "        }\n",
    "        for d in docs\n",
    "    ],\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9cdaa637-d24b-4e3f-8107-b430ce56fa25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================\n",
      "               CHAT START\n",
      "======================================\n",
      "======================================\n",
      "      INITIALIZING RETRIEVER\n",
      "======================================\n",
      "[Embedding] Provider: ollama\n",
      "[Embedding] Model: nomic-embed-text\n",
      "[Chroma] Persist directory: /Users/mona/test project of rag/vector/chroma\n",
      "[Chroma] Loading or creating Chroma DB...\n",
      "[Chroma] Vector DB ready.\n",
      "[Chroma] Returning MMR retriever.\n",
      "\n",
      "[Chat] Retrieving documents...\n",
      "[Chat] Retrieved 8 docs.\n",
      "[Chat] Calling LLM...\n",
      "[Chat] DONE.\n",
      "======================================\n",
      "\n",
      "===================== ANSWER =====================\n",
      "The document defines one information security policy: \"ACME CORPORATION â€“ INFORMATION SECURITY POLICY\".\n",
      "\n",
      "===================== UNIQUE SOURCE FILES =====================\n",
      "- ACME_Information_Security_Policy.pdf\n",
      "    path: /Users/mona/test project of rag/data/ACME_Information_Security_Policy.pdf\n",
      "    type: \n",
      "    ingested_at: \n",
      "    chunks_retrieved: 8\n"
     ]
    }
   ],
   "source": [
    "# Example RAG Query\n",
    "question = \"What policies does the document define?\"\n",
    "resp = chat(question)\n",
    "\n",
    "print(\"\\n===================== ANSWER =====================\")\n",
    "print(resp[\"answer\"])\n",
    "\n",
    "print(\"\\n===================== UNIQUE SOURCE FILES =====================\")\n",
    "\n",
    "# Build a summary of unique files and how many chunks came from each\n",
    "unique_sources = {}\n",
    "\n",
    "for d in resp[\"docs\"]:\n",
    "    fname = d[\"filename\"]\n",
    "    if fname not in unique_sources:\n",
    "        unique_sources[fname] = {\n",
    "            \"source_path\": d.get(\"source_path\", \"\"),\n",
    "            \"type\": d.get(\"type\", \"\"),\n",
    "            \"ingested_at\": d.get(\"ingested_at\", \"\"),\n",
    "            \"chunks_retrieved\": 0\n",
    "        }\n",
    "    unique_sources[fname][\"chunks_retrieved\"] += 1\n",
    "\n",
    "# Display clean summary\n",
    "for fname, meta in unique_sources.items():\n",
    "    print(f\"- {fname}\")\n",
    "    print(f\"    path: {meta['source_path']}\")\n",
    "    print(f\"    type: {meta['type']}\")\n",
    "    print(f\"    ingested_at: {meta['ingested_at']}\")\n",
    "    print(f\"    chunks_retrieved: {meta['chunks_retrieved']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a176666",
   "metadata": {},
   "source": [
    "# LangGraph RAG Pipeline\n",
    "\n",
    "This section demonstrates how the RAG workflow can be modeled as an explicit\n",
    "graph using LangGraph. Instead of running retrieval and generation as a single\n",
    "linear function call, LangGraph allows us to define a pipeline with named nodes\n",
    "and explicit transitions.\n",
    "\n",
    "This improves:\n",
    "- transparency of the RAG workflow  \n",
    "- debuggability (each node can be inspected independently)  \n",
    "- extensibility (additional nodes such as verification, re-ranking, etc.)\n",
    "\n",
    "### Pipeline Nodes\n",
    "\n",
    "1. **retrieve**  \n",
    "   Uses the retriever to fetch the most relevant document chunks for the\n",
    "   question. The output contains:\n",
    "   - retrieved documents\n",
    "   - the concatenated context string\n",
    "\n",
    "2. **generate**  \n",
    "   Calls the LLM to produce an answer using only the retrieved context. The\n",
    "   answer is stored in the graph state.\n",
    "\n",
    "The graph is then compiled into an executable `app_rag` pipeline that can be\n",
    "invoked with a question. LangGraph automatically manages state propagation\n",
    "between nodes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8f761127",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- RUNNING LANGGRAPH RAG ---\n",
      "--- RETRIEVE ---\n",
      "======================================\n",
      "      INITIALIZING RETRIEVER\n",
      "======================================\n",
      "[Embedding] Provider: ollama\n",
      "[Embedding] Model: nomic-embed-text\n",
      "[Chroma] Persist directory: /Users/mona/test project of rag/vector/chroma\n",
      "[Chroma] Loading or creating Chroma DB...\n",
      "[Chroma] Vector DB ready.\n",
      "[Chroma] Returning MMR retriever.\n",
      "\n",
      "[Retrieve] Retrieved 8 docs.\n",
      "--- GENERATE ---\n",
      "\n",
      "LangGraph Answer: According to the provided context, the CISO (Chief Information Security Officer) is responsible for:\n",
      "\n",
      "* Owning and maintaining this policy\n",
      "* Approving security standards and procedures\n",
      "* Coordinating incident response actions\n"
     ]
    }
   ],
   "source": [
    "from typing import TypedDict, List\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "# -----------------------------\n",
    "# 1. Define pipeline state\n",
    "# -----------------------------\n",
    "class RagState(TypedDict):\n",
    "    question: str\n",
    "    context: str\n",
    "    answer: str\n",
    "    docs: List[Document]\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 2. Define retrieve node\n",
    "# -----------------------------\n",
    "def retrieve(state: RagState):\n",
    "    print(\"--- RETRIEVE ---\")\n",
    "\n",
    "    question = state[\"question\"]\n",
    "    retriever = get_retriever()\n",
    "\n",
    "    docs = retriever.invoke(question)\n",
    "    print(f\"[Retrieve] Retrieved {len(docs)} docs.\")\n",
    "\n",
    "    if not docs:\n",
    "        return {\n",
    "            \"docs\": [],\n",
    "            \"context\": \"\",\n",
    "        }\n",
    "\n",
    "    context = _concat(docs)\n",
    "\n",
    "    return {\n",
    "        \"docs\": docs,\n",
    "        \"context\": context,\n",
    "    }\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 3. Define generate node\n",
    "# -----------------------------\n",
    "def generate(state: RagState):\n",
    "    print(\"--- GENERATE ---\")\n",
    "\n",
    "    question = state[\"question\"]\n",
    "    context = state.get(\"context\", \"\")\n",
    "    llm = get_chat_model()\n",
    "\n",
    "    if not context.strip():\n",
    "        return {\n",
    "            \"answer\": \"I don't know. No relevant information was retrieved.\"\n",
    "        }\n",
    "\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a strict document assistant. \"\n",
    "            \"Answer ONLY using the given context. \"\n",
    "            \"If the answer is not present, say: I don't know.\"\n",
    "        ),\n",
    "        (\"system\", \"Context:\\n{context}\"),\n",
    "        (\"human\", \"{question}\")\n",
    "    ])\n",
    "\n",
    "    chain = prompt | llm\n",
    "    resp = chain.invoke({\"context\": context, \"question\": question})\n",
    "\n",
    "    return {\"answer\": resp.content}\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 4. Build the graph\n",
    "# -----------------------------\n",
    "workflow = StateGraph(RagState)\n",
    "\n",
    "workflow.add_node(\"retrieve\", retrieve)\n",
    "workflow.add_node(\"generate\", generate)\n",
    "\n",
    "workflow.set_entry_point(\"retrieve\")\n",
    "workflow.add_edge(\"retrieve\", \"generate\")\n",
    "workflow.add_edge(\"generate\", END)\n",
    "\n",
    "app_rag = workflow.compile()\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 5. Demo run\n",
    "# -----------------------------\n",
    "print(\"--- RUNNING LANGGRAPH RAG ---\")\n",
    "inputs = {\"question\": \"What is the CISO responsible for?\"}\n",
    "result = app_rag.invoke(inputs)\n",
    "print(\"\\nLangGraph Answer:\", result[\"answer\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f58cde90-e528-48b2-a59d-0ea399844811",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "display(Image(app_rag.get_graph().draw_mermaid_png()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2c20073c-386d-4b25-bb83-8a1f137e43df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "config:\n",
      "  flowchart:\n",
      "    curve: linear\n",
      "---\n",
      "graph TD;\n",
      "\t__start__([<p>__start__</p>]):::first\n",
      "\tretrieve(retrieve)\n",
      "\tgenerate(generate)\n",
      "\t__end__([<p>__end__</p>]):::last\n",
      "\t__start__ --> retrieve;\n",
      "\tretrieve --> generate;\n",
      "\tgenerate --> __end__;\n",
      "\tclassDef default fill:#f2f0ff,line-height:1.2\n",
      "\tclassDef first fill-opacity:0\n",
      "\tclassDef last fill:#bfb6fc\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(app_rag.get_graph().draw_mermaid())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "854c7284-227d-4558-8cdd-963fb804a646",
   "metadata": {},
   "source": [
    "## 7. Answer Verification (`verify`)\n",
    "\n",
    "This function evaluates whether the generated answer is actually supported by the\n",
    "retrieved context. The goal is to detect hallucinations and assign a confidence\n",
    "score to the answer.\n",
    "\n",
    "### Verification steps\n",
    "1. Build a prompt containing:\n",
    "   - the original question\n",
    "   - the generated answer\n",
    "   - the retrieved context\n",
    "\n",
    "2. Ask the model to classify the answer as one of:\n",
    "   - `supported` â€” fully grounded in the context  \n",
    "   - `partially_supported` â€” context contains related information but does not fully justify the answer  \n",
    "   - `unsupported` â€” answer does not follow from the context  \n",
    "\n",
    "3. Convert the classification into a numerical confidence score:\n",
    "   - `supported` â†’ **1.0**  \n",
    "   - `partially_supported` â†’ **0.5**  \n",
    "   - `unsupported` â†’ **0.0**  \n",
    "\n",
    "This provides a lightweight hallucination-detection method that estimates the\n",
    "reliability of the answer derived through the RAG pipeline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d5d57ba6-97a8-42d3-a6e8-f6a13226d648",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "def verify(question: str, answer: str, context: str):\n",
    "    print(\"\\n======================================\")\n",
    "    print(\"             VERIFY START\")\n",
    "    print(\"======================================\")\n",
    "\n",
    "    llm = get_chat_model()\n",
    "\n",
    "    vprompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\n",
    "                \"system\",\n",
    "                (\n",
    "                    \"You are a strict answer verifier. Determine whether the ANSWER \"\n",
    "                    \"is supported by the CONTEXT. You MUST return exactly one of:\\n\"\n",
    "                    \"- supported\\n\"\n",
    "                    \"- partially_supported\\n\"\n",
    "                    \"- unsupported\\n\"\n",
    "                    \"Do NOT explain. Do NOT add anything else.\"\n",
    "                )\n",
    "            ),\n",
    "            (\n",
    "                \"human\",\n",
    "                \"QUESTION:\\n{q}\\n\\nANSWER:\\n{a}\\n\\nCONTEXT:\\n{c}\\n\\nReturn ONLY the label:\"\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    chain = vprompt | llm\n",
    "\n",
    "    print(\"[Verify] Calling verifier model...\")\n",
    "    raw_output = chain.invoke(\n",
    "        {\"q\": question, \"a\": answer, \"c\": context}\n",
    "    ).content.strip().lower()\n",
    "\n",
    "    print(f\"[Verify] Raw model output: '{raw_output}'\")\n",
    "\n",
    "    # ---------- Normalization ----------\n",
    "    verdict = \"unsupported\"  # safe default\n",
    "\n",
    "    if raw_output.startswith(\"supported\") and not raw_output.startswith(\"partially\"):\n",
    "        verdict = \"supported\"\n",
    "    elif raw_output.startswith(\"partially\"):\n",
    "        verdict = \"partially_supported\"\n",
    "\n",
    "    # ---------- Confidence scoring ----------\n",
    "    confidence_map = {\n",
    "        \"supported\": 1.0,\n",
    "        \"partially_supported\": 0.5,\n",
    "        \"unsupported\": 0.0,\n",
    "    }\n",
    "    confidence = confidence_map[verdict]\n",
    "\n",
    "    print(f\"[Verify] Verdict: {verdict}\")\n",
    "    print(f\"[Verify] Confidence: {confidence}\")\n",
    "    print(\"======================================\")\n",
    "\n",
    "    return {\n",
    "        \"verdict\": verdict,\n",
    "        \"confidence\": confidence,\n",
    "        \"raw_model_output\": raw_output,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e93b5cf6-2886-4b27-8004-b4d47c6a7334",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================\n",
      "               CHAT START\n",
      "======================================\n",
      "======================================\n",
      "      INITIALIZING RETRIEVER\n",
      "======================================\n",
      "[Embedding] Provider: ollama\n",
      "[Embedding] Model: nomic-embed-text\n",
      "[Chroma] Persist directory: /Users/mona/test project of rag/vector/chroma\n",
      "[Chroma] Loading or creating Chroma DB...\n",
      "[Chroma] Vector DB ready.\n",
      "[Chroma] Returning MMR retriever.\n",
      "\n",
      "[Chat] Retrieving documents...\n",
      "[Chat] Retrieved 8 docs.\n",
      "[Chat] Calling LLM...\n",
      "[Chat] DONE.\n",
      "======================================\n",
      "\n",
      "======================================\n",
      "             VERIFY START\n",
      "======================================\n",
      "[Verify] Calling verifier model...\n",
      "[Verify] Raw model output: 'supported'\n",
      "[Verify] Verdict: supported\n",
      "[Verify] Confidence: 1.0\n",
      "======================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'verdict': 'supported', 'confidence': 1.0, 'raw_model_output': 'supported'}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resp = chat(\"Does the policy include incident response?\")\n",
    "answer = resp[\"answer\"]\n",
    "context = resp[\"context\"]\n",
    "\n",
    "verify(\"Does the policy include incident response?\", answer, context)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09092517-2736-48d4-a5c1-f93a3f67e5dd",
   "metadata": {},
   "source": [
    "## 8. Requirement Validation (`validate`)\n",
    "\n",
    "This function implements the core compliance-checking capability of the framework.\n",
    "Given a list of requirement statements (e.g., ISO 27001 controls), the function\n",
    "automatically determines whether each requirement is present in the document.\n",
    "\n",
    "### Validation steps\n",
    "\n",
    "For every requirement:\n",
    "\n",
    "1. **Retrieve relevant chunks**  \n",
    "   Use the retriever to fetch the most semantically similar document fragments.\n",
    "\n",
    "2. **Assemble context**  \n",
    "   Combine the retrieved chunks into a concise context string.\n",
    "\n",
    "3. **Ask the LLM to decide**  \n",
    "   Prompt the model with the requirement and the context, instructing it to answer:\n",
    "   - `\"yes\"` if the requirement is clearly supported\n",
    "   - `\"no\"` otherwise  \n",
    "   The model must also provide a short justification.\n",
    "\n",
    "4. **Interpret the response**  \n",
    "   - If the answer begins with `\"yes\"` â†’ `present = True`  \n",
    "   - Otherwise â†’ `present = False`  \n",
    "\n",
    "5. **Return structured results**  \n",
    "   Each requirement returns:\n",
    "   - `requirement` â€” the original text  \n",
    "   - `present` â€” boolean indicating whether it is found  \n",
    "   - `raw` â€” the full LLM justification  \n",
    "   - `docs` â€” metadata for retrieved supporting chunks  \n",
    "\n",
    "### Purpose\n",
    "\n",
    "This provides an automated way to validate whether a policy, specification, or\n",
    "technical document contains the required content. It directly supports the thesis\n",
    "goal of automatic document compliance checking.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f6ed74d7-79ba-4fa7-9ed1-f2bcc2631a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "def validate(requirements):\n",
    "    print(\"\\n======================================\")\n",
    "    print(\"           VALIDATION START\")\n",
    "    print(\"======================================\")\n",
    "\n",
    "    retriever = get_retriever()\n",
    "    llm = get_chat_model()\n",
    "\n",
    "    vprompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\n",
    "                \"system\",\n",
    "                (\n",
    "                    \"You are a strict compliance validator. Determine whether the \"\n",
    "                    \"REQUIREMENT is present in the document based ONLY on the CONTEXT.\\n\\n\"\n",
    "                    \"Return exactly one of the following formats:\\n\"\n",
    "                    \"  yes: <short justification>\\n\"\n",
    "                    \"  no: <short justification>\\n\\n\"\n",
    "                    \"No extra sentences. No explanations beyond the short justification.\"\n",
    "                )\n",
    "            ),\n",
    "            (\n",
    "                \"human\",\n",
    "                \"Requirement:\\n{req}\\n\\nContext:\\n{ctx}\\n\\nReturn your decision:\"\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    chain = vprompt | llm\n",
    "    results = []\n",
    "\n",
    "    for req in requirements:\n",
    "        print(\"\\n--------------------------------------\")\n",
    "        print(f\"[Validate] Requirement: {req}\")\n",
    "\n",
    "        # modern API: retriever.invoke()\n",
    "        docs = retriever.invoke(req)\n",
    "        print(f\"[Validate] Retrieved {len(docs)} docs\")\n",
    "\n",
    "        if not docs:\n",
    "            print(\"[Validate] No context â†’ auto 'no'\")\n",
    "            results.append({\n",
    "                \"requirement\": req,\n",
    "                \"present\": False,\n",
    "                \"raw\": \"no: no supporting context found\",\n",
    "                \"confidence\": 0.0,\n",
    "                \"docs\": [],\n",
    "            })\n",
    "            continue\n",
    "\n",
    "        ctx = _concat(docs)\n",
    "\n",
    "        # Call model\n",
    "        resp = chain.invoke({\"req\": req, \"ctx\": ctx}).content.strip()\n",
    "        raw = resp.lower()\n",
    "\n",
    "        print(f\"[Validate] Raw LLM output: {resp}\")\n",
    "\n",
    "        # ---------- normalize yes/no ----------\n",
    "        if raw.startswith(\"yes\"):\n",
    "            present = True\n",
    "            confidence = 1.0\n",
    "        elif raw.startswith(\"no\"):\n",
    "            present = False\n",
    "            confidence = 0.0\n",
    "        else:\n",
    "            # safety fallback\n",
    "            present = False\n",
    "            confidence = 0.0\n",
    "            resp = \"no: unclear model output\"\n",
    "\n",
    "        print(f\"[Validate] Present? {present}\")\n",
    "\n",
    "        # metadata\n",
    "        doc_meta = [\n",
    "            {\n",
    "                \"source_path\": d.metadata.get(\"source_path\"),\n",
    "                \"filename\": d.metadata.get(\"filename\"),\n",
    "                \"ingested_at\": d.metadata.get(\"ingested_at\"),\n",
    "                \"type\": d.metadata.get(\"type\"),\n",
    "            }\n",
    "            for d in docs\n",
    "        ]\n",
    "\n",
    "        results.append(\n",
    "            {\n",
    "                \"requirement\": req,\n",
    "                \"present\": present,\n",
    "                \"raw\": resp,\n",
    "                \"confidence\": confidence,\n",
    "                \"docs\": doc_meta,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    print(\"\\n======================================\")\n",
    "    print(\"           VALIDATION END\")\n",
    "    print(\"======================================\")\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "07127b40-71da-4c42-9f29-53c10d51264e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================\n",
      "           VALIDATION START\n",
      "======================================\n",
      "======================================\n",
      "      INITIALIZING RETRIEVER\n",
      "======================================\n",
      "[Embedding] Provider: ollama\n",
      "[Embedding] Model: nomic-embed-text\n",
      "[Chroma] Persist directory: /Users/mona/test project of rag/vector/chroma\n",
      "[Chroma] Loading or creating Chroma DB...\n",
      "[Chroma] Vector DB ready.\n",
      "[Chroma] Returning MMR retriever.\n",
      "\n",
      "\n",
      "--------------------------------------\n",
      "[Validate] Requirement: The policy must include an incident response process.\n",
      "[Validate] Retrieved 8 docs\n",
      "[Validate] Raw LLM output: yes: The policy includes an incident response process, which is outlined in section 8.3 Incident Response Process.\n",
      "[Validate] Present? True\n",
      "\n",
      "--------------------------------------\n",
      "[Validate] Requirement: The policy must define roles and responsibilities.\n",
      "[Validate] Retrieved 8 docs\n",
      "[Validate] Raw LLM output: yes: The requirement is present in section 1. Purpose, which states \"to define the principles, responsibilities, and controls required to protect ACME Corporation's information assets...\"\n",
      "[Validate] Present? True\n",
      "\n",
      "--------------------------------------\n",
      "[Validate] Requirement: The policy must describe backup and recovery.\n",
      "[Validate] Retrieved 8 docs\n",
      "[Validate] Raw LLM output: yes: The requirement to describe backup and recovery is explicitly stated in section 9 of the document, which covers \"Backup and Recovery\" and outlines specific requirements for backup frequency, protection, and testing.\n",
      "[Validate] Present? True\n",
      "\n",
      "--------------------------------------\n",
      "[Validate] Requirement: The policy must define VPN usage rules.\n",
      "[Validate] Retrieved 8 docs\n",
      "[Validate] Raw LLM output: no: The requirement \"The policy must define VPN usage rules\" is not present in the document.\n",
      "[Validate] Present? False\n",
      "\n",
      "======================================\n",
      "           VALIDATION END\n",
      "======================================\n",
      "\n",
      "REQ: The policy must include an incident response process.\n",
      "PRESENT: True\n",
      "RAW: yes: The policy includes an incident response process, which is outlined in section 8.3 Incident Response Process.\n",
      "FILES: {'ACME_Information_Security_Policy.pdf'}\n",
      "\n",
      "REQ: The policy must define roles and responsibilities.\n",
      "PRESENT: True\n",
      "RAW: yes: The requirement is present in section 1. Purpose, which states \"to define the principles, responsibilities, and controls required to protect ACME Corporation's information assets...\"\n",
      "FILES: {'ACME_Information_Security_Policy.pdf'}\n",
      "\n",
      "REQ: The policy must describe backup and recovery.\n",
      "PRESENT: True\n",
      "RAW: yes: The requirement to describe backup and recovery is explicitly stated in section 9 of the document, which covers \"Backup and Recovery\" and outlines specific requirements for backup frequency, protection, and testing.\n",
      "FILES: {'ACME_Information_Security_Policy.pdf'}\n",
      "\n",
      "REQ: The policy must define VPN usage rules.\n",
      "PRESENT: False\n",
      "RAW: no: The requirement \"The policy must define VPN usage rules\" is not present in the document.\n",
      "FILES: {'ACME_Information_Security_Policy.pdf'}\n"
     ]
    }
   ],
   "source": [
    "reqs = [\n",
    "    \"The policy must include an incident response process.\",\n",
    "    \"The policy must define roles and responsibilities.\",\n",
    "    \"The policy must describe backup and recovery.\",\n",
    "    \"The policy must define VPN usage rules.\",\n",
    "]\n",
    "\n",
    "results = validate(reqs)\n",
    "\n",
    "for r in results:\n",
    "    print(\"\\nREQ:\", r[\"requirement\"])\n",
    "    print(\"PRESENT:\", r[\"present\"])\n",
    "    print(\"RAW:\", r[\"raw\"])\n",
    "    print(\"FILES:\", {d[\"filename\"] for d in r[\"docs\"]})\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
