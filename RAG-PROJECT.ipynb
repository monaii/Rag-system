{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69df0622-d7a1-43eb-82b2-b72576c1e701",
   "metadata": {},
   "source": [
    "Project Introduction\n",
    "\n",
    "This project implements a lightweight AI framework for automatic search and validation of documentary content using Retrieval-Augmented Generation (RAG).\n",
    "It is designed for corporate environments where large PDFs, policies, procedures, and technical documents must be checked for the presence of specific requirements or compliance criteria.\n",
    "\n",
    "The framework provides:\n",
    "\n",
    "Document ingestion from PDF, DOCX, PPTX\n",
    "\n",
    "Semantic chunking + embedding using OpenAI or local LLMs (Ollama)\n",
    "\n",
    "Persistent vector database built with Chroma\n",
    "\n",
    "RAG-based question answering grounded strictly on retrieved context\n",
    "\n",
    "Requirement validation module that returns yes / no with justification\n",
    "\n",
    "Hallucination-mitigation via context-supported verification\n",
    "\n",
    "CLI and Flask API interfaces for interaction and integration\n",
    "\n",
    "The goal is to demonstrate how modern LLMs + semantic search can automate manual document review tasks and enable consistent, auditable compliance checking."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "967dd1a4-bb3b-46f9-b807-8a525304ef1c",
   "metadata": {},
   "source": [
    "# 0. Environment Setup\n",
    "\n",
    "Before running any part of this RAG Document Validation framework, we need to\n",
    "install all required Python libraries. These libraries support:\n",
    "\n",
    "- LLM providers (OpenAI, Mistral, Ollama via LangChain)\n",
    "- Vector stores (FAISS, Chroma, Pinecone)\n",
    "- RAG pipeline tools (LangChain, LangGraph, LlamaIndex)\n",
    "- Evaluation frameworks (RAGAS, datasets, evaluate)\n",
    "- API server (Flask)\n",
    "- Environment variable loading (python-dotenv)\n",
    "\n",
    "The project includes a `requirements.txt` file containing all needed\n",
    "dependencies. We install them using:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c160bed7-336e-45c8-94cf-49917d735807",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-dotenv in /opt/anaconda3/lib/python3.13/site-packages (from -r requirements.txt (line 1)) (1.1.0)\n",
      "Requirement already satisfied: langchain in /opt/anaconda3/lib/python3.13/site-packages (from -r requirements.txt (line 2)) (0.3.7)\n",
      "Requirement already satisfied: langchain-core in /opt/anaconda3/lib/python3.13/site-packages (from -r requirements.txt (line 3)) (0.3.63)\n",
      "Requirement already satisfied: langchain-openai in /opt/anaconda3/lib/python3.13/site-packages (from -r requirements.txt (line 4)) (0.3.19)\n",
      "Requirement already satisfied: langchain-chroma in /opt/anaconda3/lib/python3.13/site-packages (from -r requirements.txt (line 5)) (0.2.2)\n",
      "Requirement already satisfied: langchain-community in /opt/anaconda3/lib/python3.13/site-packages (from -r requirements.txt (line 6)) (0.3.7)\n",
      "Requirement already satisfied: langchain-mistralai in /opt/anaconda3/lib/python3.13/site-packages (from -r requirements.txt (line 7)) (0.2.10)\n",
      "Requirement already satisfied: langgraph in /opt/anaconda3/lib/python3.13/site-packages (from -r requirements.txt (line 8)) (0.5.4)\n",
      "Requirement already satisfied: chromadb in /opt/anaconda3/lib/python3.13/site-packages (from -r requirements.txt (line 9)) (0.6.3)\n",
      "Requirement already satisfied: faiss-cpu in /opt/anaconda3/lib/python3.13/site-packages (from -r requirements.txt (line 10)) (1.13.0)\n",
      "Requirement already satisfied: pinecone-client in /opt/anaconda3/lib/python3.13/site-packages (from -r requirements.txt (line 11)) (6.0.0)\n",
      "Requirement already satisfied: langchain-pinecone in /opt/anaconda3/lib/python3.13/site-packages (from -r requirements.txt (line 12)) (0.2.13)\n",
      "Requirement already satisfied: flask in /opt/anaconda3/lib/python3.13/site-packages (from -r requirements.txt (line 13)) (3.1.0)\n",
      "Requirement already satisfied: ragas in /opt/anaconda3/lib/python3.13/site-packages (from -r requirements.txt (line 14)) (0.4.0)\n",
      "Requirement already satisfied: datasets in /opt/anaconda3/lib/python3.13/site-packages (from -r requirements.txt (line 15)) (4.4.1)\n",
      "Requirement already satisfied: evaluate in /opt/anaconda3/lib/python3.13/site-packages (from -r requirements.txt (line 16)) (0.4.6)\n",
      "Requirement already satisfied: llama-index in /opt/anaconda3/lib/python3.13/site-packages (from -r requirements.txt (line 17)) (0.14.10)\n",
      "Requirement already satisfied: llama-index-vector-stores-chroma in /opt/anaconda3/lib/python3.13/site-packages (from -r requirements.txt (line 18)) (0.5.3)\n",
      "Requirement already satisfied: llama-index-embeddings-openai in /opt/anaconda3/lib/python3.13/site-packages (from -r requirements.txt (line 19)) (0.5.1)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /opt/anaconda3/lib/python3.13/site-packages (from langchain->-r requirements.txt (line 2)) (6.0.2)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/anaconda3/lib/python3.13/site-packages (from langchain->-r requirements.txt (line 2)) (2.0.35)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /opt/anaconda3/lib/python3.13/site-packages (from langchain->-r requirements.txt (line 2)) (3.11.10)\n",
      "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.0 in /opt/anaconda3/lib/python3.13/site-packages (from langchain->-r requirements.txt (line 2)) (0.3.8)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /opt/anaconda3/lib/python3.13/site-packages (from langchain->-r requirements.txt (line 2)) (0.1.147)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.26.0 in /opt/anaconda3/lib/python3.13/site-packages (from langchain->-r requirements.txt (line 2)) (1.26.4)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /opt/anaconda3/lib/python3.13/site-packages (from langchain->-r requirements.txt (line 2)) (2.12.5)\n",
      "Requirement already satisfied: requests<3,>=2 in /opt/anaconda3/lib/python3.13/site-packages (from langchain->-r requirements.txt (line 2)) (2.32.5)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /opt/anaconda3/lib/python3.13/site-packages (from langchain->-r requirements.txt (line 2)) (9.0.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/anaconda3/lib/python3.13/site-packages (from langchain-core->-r requirements.txt (line 3)) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /opt/anaconda3/lib/python3.13/site-packages (from langchain-core->-r requirements.txt (line 3)) (24.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /opt/anaconda3/lib/python3.13/site-packages (from langchain-core->-r requirements.txt (line 3)) (4.15.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /opt/anaconda3/lib/python3.13/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain->-r requirements.txt (line 2)) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/anaconda3/lib/python3.13/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain->-r requirements.txt (line 2)) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/anaconda3/lib/python3.13/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain->-r requirements.txt (line 2)) (24.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/anaconda3/lib/python3.13/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain->-r requirements.txt (line 2)) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/anaconda3/lib/python3.13/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain->-r requirements.txt (line 2)) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/anaconda3/lib/python3.13/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain->-r requirements.txt (line 2)) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /opt/anaconda3/lib/python3.13/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain->-r requirements.txt (line 2)) (1.18.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /opt/anaconda3/lib/python3.13/site-packages (from jsonpatch<2.0,>=1.33->langchain-core->-r requirements.txt (line 3)) (2.1)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /opt/anaconda3/lib/python3.13/site-packages (from langsmith<0.2.0,>=0.1.17->langchain->-r requirements.txt (line 2)) (0.28.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /opt/anaconda3/lib/python3.13/site-packages (from langsmith<0.2.0,>=0.1.17->langchain->-r requirements.txt (line 2)) (3.11.4)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /opt/anaconda3/lib/python3.13/site-packages (from langsmith<0.2.0,>=0.1.17->langchain->-r requirements.txt (line 2)) (1.0.0)\n",
      "Requirement already satisfied: anyio in /opt/anaconda3/lib/python3.13/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain->-r requirements.txt (line 2)) (4.7.0)\n",
      "Requirement already satisfied: certifi in /opt/anaconda3/lib/python3.13/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain->-r requirements.txt (line 2)) (2025.11.12)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/anaconda3/lib/python3.13/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain->-r requirements.txt (line 2)) (1.0.9)\n",
      "Requirement already satisfied: idna in /opt/anaconda3/lib/python3.13/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain->-r requirements.txt (line 2)) (3.7)\n",
      "Requirement already satisfied: h11>=0.16 in /opt/anaconda3/lib/python3.13/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain->-r requirements.txt (line 2)) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/anaconda3/lib/python3.13/site-packages (from pydantic<3.0.0,>=2.7.4->langchain->-r requirements.txt (line 2)) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in /opt/anaconda3/lib/python3.13/site-packages (from pydantic<3.0.0,>=2.7.4->langchain->-r requirements.txt (line 2)) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /opt/anaconda3/lib/python3.13/site-packages (from pydantic<3.0.0,>=2.7.4->langchain->-r requirements.txt (line 2)) (0.4.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/anaconda3/lib/python3.13/site-packages (from requests<3,>=2->langchain->-r requirements.txt (line 2)) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.13/site-packages (from requests<3,>=2->langchain->-r requirements.txt (line 2)) (2.3.0)\n",
      "Requirement already satisfied: openai<2.0.0,>=1.68.2 in /opt/anaconda3/lib/python3.13/site-packages (from langchain-openai->-r requirements.txt (line 4)) (1.109.1)\n",
      "Requirement already satisfied: tiktoken<1,>=0.7 in /opt/anaconda3/lib/python3.13/site-packages (from langchain-openai->-r requirements.txt (line 4)) (0.12.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /opt/anaconda3/lib/python3.13/site-packages (from openai<2.0.0,>=1.68.2->langchain-openai->-r requirements.txt (line 4)) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /opt/anaconda3/lib/python3.13/site-packages (from openai<2.0.0,>=1.68.2->langchain-openai->-r requirements.txt (line 4)) (0.10.0)\n",
      "Requirement already satisfied: sniffio in /opt/anaconda3/lib/python3.13/site-packages (from openai<2.0.0,>=1.68.2->langchain-openai->-r requirements.txt (line 4)) (1.3.0)\n",
      "Requirement already satisfied: tqdm>4 in /opt/anaconda3/lib/python3.13/site-packages (from openai<2.0.0,>=1.68.2->langchain-openai->-r requirements.txt (line 4)) (4.67.1)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /opt/anaconda3/lib/python3.13/site-packages (from tiktoken<1,>=0.7->langchain-openai->-r requirements.txt (line 4)) (2024.11.6)\n",
      "Requirement already satisfied: build>=1.0.3 in /opt/anaconda3/lib/python3.13/site-packages (from chromadb->-r requirements.txt (line 9)) (1.3.0)\n",
      "Requirement already satisfied: chroma-hnswlib==0.7.6 in /opt/anaconda3/lib/python3.13/site-packages (from chromadb->-r requirements.txt (line 9)) (0.7.6)\n",
      "Requirement already satisfied: fastapi>=0.95.2 in /opt/anaconda3/lib/python3.13/site-packages (from chromadb->-r requirements.txt (line 9)) (0.124.0)\n",
      "Requirement already satisfied: uvicorn>=0.18.3 in /opt/anaconda3/lib/python3.13/site-packages (from uvicorn[standard]>=0.18.3->chromadb->-r requirements.txt (line 9)) (0.38.0)\n",
      "Requirement already satisfied: posthog>=2.4.0 in /opt/anaconda3/lib/python3.13/site-packages (from chromadb->-r requirements.txt (line 9)) (5.4.0)\n",
      "Requirement already satisfied: onnxruntime>=1.14.1 in /opt/anaconda3/lib/python3.13/site-packages (from chromadb->-r requirements.txt (line 9)) (1.23.2)\n",
      "Requirement already satisfied: opentelemetry-api>=1.2.0 in /opt/anaconda3/lib/python3.13/site-packages (from chromadb->-r requirements.txt (line 9)) (1.39.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in /opt/anaconda3/lib/python3.13/site-packages (from chromadb->-r requirements.txt (line 9)) (1.39.0)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-fastapi>=0.41b0 in /opt/anaconda3/lib/python3.13/site-packages (from chromadb->-r requirements.txt (line 9)) (0.60b0)\n",
      "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /opt/anaconda3/lib/python3.13/site-packages (from chromadb->-r requirements.txt (line 9)) (1.39.0)\n",
      "Requirement already satisfied: tokenizers>=0.13.2 in /opt/anaconda3/lib/python3.13/site-packages (from chromadb->-r requirements.txt (line 9)) (0.22.1)\n",
      "Requirement already satisfied: pypika>=0.48.9 in /opt/anaconda3/lib/python3.13/site-packages (from chromadb->-r requirements.txt (line 9)) (0.48.9)\n",
      "Requirement already satisfied: overrides>=7.3.1 in /opt/anaconda3/lib/python3.13/site-packages (from chromadb->-r requirements.txt (line 9)) (7.4.0)\n",
      "Requirement already satisfied: importlib-resources in /opt/anaconda3/lib/python3.13/site-packages (from chromadb->-r requirements.txt (line 9)) (6.5.2)\n",
      "Requirement already satisfied: grpcio>=1.58.0 in /opt/anaconda3/lib/python3.13/site-packages (from chromadb->-r requirements.txt (line 9)) (1.76.0)\n",
      "Requirement already satisfied: bcrypt>=4.0.1 in /opt/anaconda3/lib/python3.13/site-packages (from chromadb->-r requirements.txt (line 9)) (4.3.0)\n",
      "Requirement already satisfied: typer>=0.9.0 in /opt/anaconda3/lib/python3.13/site-packages (from chromadb->-r requirements.txt (line 9)) (0.9.0)\n",
      "Requirement already satisfied: kubernetes>=28.1.0 in /opt/anaconda3/lib/python3.13/site-packages (from chromadb->-r requirements.txt (line 9)) (34.1.0)\n",
      "Requirement already satisfied: mmh3>=4.0.1 in /opt/anaconda3/lib/python3.13/site-packages (from chromadb->-r requirements.txt (line 9)) (5.2.0)\n",
      "Requirement already satisfied: rich>=10.11.0 in /opt/anaconda3/lib/python3.13/site-packages (from chromadb->-r requirements.txt (line 9)) (13.9.4)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /opt/anaconda3/lib/python3.13/site-packages (from langchain-community->-r requirements.txt (line 6)) (0.6.7)\n",
      "Requirement already satisfied: httpx-sse<0.5.0,>=0.4.0 in /opt/anaconda3/lib/python3.13/site-packages (from langchain-community->-r requirements.txt (line 6)) (0.4.3)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /opt/anaconda3/lib/python3.13/site-packages (from langchain-community->-r requirements.txt (line 6)) (2.12.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /opt/anaconda3/lib/python3.13/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community->-r requirements.txt (line 6)) (3.26.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /opt/anaconda3/lib/python3.13/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community->-r requirements.txt (line 6)) (0.9.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /opt/anaconda3/lib/python3.13/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community->-r requirements.txt (line 6)) (1.0.0)\n",
      "Requirement already satisfied: huggingface-hub<2.0,>=0.16.4 in /opt/anaconda3/lib/python3.13/site-packages (from tokenizers>=0.13.2->chromadb->-r requirements.txt (line 9)) (0.36.0)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.13/site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers>=0.13.2->chromadb->-r requirements.txt (line 9)) (3.17.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/anaconda3/lib/python3.13/site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers>=0.13.2->chromadb->-r requirements.txt (line 9)) (2025.3.2)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /opt/anaconda3/lib/python3.13/site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers>=0.13.2->chromadb->-r requirements.txt (line 9)) (1.2.0)\n",
      "Requirement already satisfied: langgraph-checkpoint<3.0.0,>=2.1.0 in /opt/anaconda3/lib/python3.13/site-packages (from langgraph->-r requirements.txt (line 8)) (2.1.2)\n",
      "Requirement already satisfied: langgraph-prebuilt<0.6.0,>=0.5.0 in /opt/anaconda3/lib/python3.13/site-packages (from langgraph->-r requirements.txt (line 8)) (0.5.1)\n",
      "Requirement already satisfied: langgraph-sdk<0.2.0,>=0.1.42 in /opt/anaconda3/lib/python3.13/site-packages (from langgraph->-r requirements.txt (line 8)) (0.1.74)\n",
      "Requirement already satisfied: xxhash>=3.5.0 in /opt/anaconda3/lib/python3.13/site-packages (from langgraph->-r requirements.txt (line 8)) (3.6.0)\n",
      "Requirement already satisfied: ormsgpack>=1.10.0 in /opt/anaconda3/lib/python3.13/site-packages (from langgraph-checkpoint<3.0.0,>=2.1.0->langgraph->-r requirements.txt (line 8)) (1.12.0)\n",
      "Requirement already satisfied: pinecone-plugin-interface<0.0.8,>=0.0.7 in /opt/anaconda3/lib/python3.13/site-packages (from pinecone-client->-r requirements.txt (line 11)) (0.0.7)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in /opt/anaconda3/lib/python3.13/site-packages (from pinecone-client->-r requirements.txt (line 11)) (2.9.0.post0)\n",
      "Requirement already satisfied: pinecone<8.0.0,>=6.0.0 in /opt/anaconda3/lib/python3.13/site-packages (from pinecone[asyncio]<8.0.0,>=6.0.0->langchain-pinecone->-r requirements.txt (line 12)) (7.3.0)\n",
      "Requirement already satisfied: simsimd>=5.9.11 in /opt/anaconda3/lib/python3.13/site-packages (from langchain-pinecone->-r requirements.txt (line 12)) (6.5.3)\n",
      "Requirement already satisfied: pinecone-plugin-assistant<2.0.0,>=1.6.0 in /opt/anaconda3/lib/python3.13/site-packages (from pinecone<8.0.0,>=6.0.0->pinecone[asyncio]<8.0.0,>=6.0.0->langchain-pinecone->-r requirements.txt (line 12)) (1.8.0)\n",
      "Requirement already satisfied: aiohttp-retry<3.0.0,>=2.9.1 in /opt/anaconda3/lib/python3.13/site-packages (from pinecone[asyncio]<8.0.0,>=6.0.0->langchain-pinecone->-r requirements.txt (line 12)) (2.9.1)\n",
      "Requirement already satisfied: Werkzeug>=3.1 in /opt/anaconda3/lib/python3.13/site-packages (from flask->-r requirements.txt (line 13)) (3.1.3)\n",
      "Requirement already satisfied: Jinja2>=3.1.2 in /opt/anaconda3/lib/python3.13/site-packages (from flask->-r requirements.txt (line 13)) (3.1.6)\n",
      "Requirement already satisfied: itsdangerous>=2.2 in /opt/anaconda3/lib/python3.13/site-packages (from flask->-r requirements.txt (line 13)) (2.2.0)\n",
      "Requirement already satisfied: click>=8.1.3 in /opt/anaconda3/lib/python3.13/site-packages (from flask->-r requirements.txt (line 13)) (8.1.8)\n",
      "Requirement already satisfied: blinker>=1.9 in /opt/anaconda3/lib/python3.13/site-packages (from flask->-r requirements.txt (line 13)) (1.9.0)\n",
      "Requirement already satisfied: nest-asyncio in /opt/anaconda3/lib/python3.13/site-packages (from ragas->-r requirements.txt (line 14)) (1.6.0)\n",
      "Requirement already satisfied: appdirs in /opt/anaconda3/lib/python3.13/site-packages (from ragas->-r requirements.txt (line 14)) (1.4.4)\n",
      "Requirement already satisfied: diskcache>=5.6.3 in /opt/anaconda3/lib/python3.13/site-packages (from ragas->-r requirements.txt (line 14)) (5.6.3)\n",
      "Requirement already satisfied: instructor in /opt/anaconda3/lib/python3.13/site-packages (from ragas->-r requirements.txt (line 14)) (1.12.0)\n",
      "Requirement already satisfied: pillow>=10.4.0 in /opt/anaconda3/lib/python3.13/site-packages (from ragas->-r requirements.txt (line 14)) (11.1.0)\n",
      "Requirement already satisfied: networkx in /opt/anaconda3/lib/python3.13/site-packages (from ragas->-r requirements.txt (line 14)) (3.4.2)\n",
      "Requirement already satisfied: scikit-network in /opt/anaconda3/lib/python3.13/site-packages (from ragas->-r requirements.txt (line 14)) (0.33.5)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in /opt/anaconda3/lib/python3.13/site-packages (from datasets->-r requirements.txt (line 15)) (22.0.0)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in /opt/anaconda3/lib/python3.13/site-packages (from datasets->-r requirements.txt (line 15)) (0.4.0)\n",
      "Requirement already satisfied: pandas in /opt/anaconda3/lib/python3.13/site-packages (from datasets->-r requirements.txt (line 15)) (2.2.3)\n",
      "Requirement already satisfied: multiprocess<0.70.19 in /opt/anaconda3/lib/python3.13/site-packages (from datasets->-r requirements.txt (line 15)) (0.70.18)\n",
      "Requirement already satisfied: llama-index-cli<0.6,>=0.5.0 in /opt/anaconda3/lib/python3.13/site-packages (from llama-index->-r requirements.txt (line 17)) (0.5.3)\n",
      "Requirement already satisfied: llama-index-core<0.15.0,>=0.14.10 in /opt/anaconda3/lib/python3.13/site-packages (from llama-index->-r requirements.txt (line 17)) (0.14.10)\n",
      "Requirement already satisfied: llama-index-indices-managed-llama-cloud>=0.4.0 in /opt/anaconda3/lib/python3.13/site-packages (from llama-index->-r requirements.txt (line 17)) (0.9.4)\n",
      "Requirement already satisfied: llama-index-llms-openai<0.7,>=0.6.0 in /opt/anaconda3/lib/python3.13/site-packages (from llama-index->-r requirements.txt (line 17)) (0.6.10)\n",
      "Requirement already satisfied: llama-index-readers-file<0.6,>=0.5.0 in /opt/anaconda3/lib/python3.13/site-packages (from llama-index->-r requirements.txt (line 17)) (0.5.5)\n",
      "Requirement already satisfied: llama-index-readers-llama-parse>=0.4.0 in /opt/anaconda3/lib/python3.13/site-packages (from llama-index->-r requirements.txt (line 17)) (0.5.1)\n",
      "Requirement already satisfied: nltk>3.8.1 in /opt/anaconda3/lib/python3.13/site-packages (from llama-index->-r requirements.txt (line 17)) (3.9.1)\n",
      "Requirement already satisfied: aiosqlite in /opt/anaconda3/lib/python3.13/site-packages (from llama-index-core<0.15.0,>=0.14.10->llama-index->-r requirements.txt (line 17)) (0.21.0)\n",
      "Requirement already satisfied: banks<3,>=2.2.0 in /opt/anaconda3/lib/python3.13/site-packages (from llama-index-core<0.15.0,>=0.14.10->llama-index->-r requirements.txt (line 17)) (2.2.0)\n",
      "Requirement already satisfied: deprecated>=1.2.9.3 in /opt/anaconda3/lib/python3.13/site-packages (from llama-index-core<0.15.0,>=0.14.10->llama-index->-r requirements.txt (line 17)) (1.2.18)\n",
      "Requirement already satisfied: dirtyjson<2,>=1.0.8 in /opt/anaconda3/lib/python3.13/site-packages (from llama-index-core<0.15.0,>=0.14.10->llama-index->-r requirements.txt (line 17)) (1.0.8)\n",
      "Requirement already satisfied: filetype<2,>=1.2.0 in /opt/anaconda3/lib/python3.13/site-packages (from llama-index-core<0.15.0,>=0.14.10->llama-index->-r requirements.txt (line 17)) (1.2.0)\n",
      "Requirement already satisfied: llama-index-workflows!=2.9.0,<3,>=2 in /opt/anaconda3/lib/python3.13/site-packages (from llama-index-core<0.15.0,>=0.14.10->llama-index->-r requirements.txt (line 17)) (2.11.5)\n",
      "Requirement already satisfied: platformdirs in /opt/anaconda3/lib/python3.13/site-packages (from llama-index-core<0.15.0,>=0.14.10->llama-index->-r requirements.txt (line 17)) (4.3.7)\n",
      "Requirement already satisfied: setuptools>=80.9.0 in /opt/anaconda3/lib/python3.13/site-packages (from llama-index-core<0.15.0,>=0.14.10->llama-index->-r requirements.txt (line 17)) (80.9.0)\n",
      "Requirement already satisfied: wrapt in /opt/anaconda3/lib/python3.13/site-packages (from llama-index-core<0.15.0,>=0.14.10->llama-index->-r requirements.txt (line 17)) (1.17.0)\n",
      "Requirement already satisfied: griffe in /opt/anaconda3/lib/python3.13/site-packages (from banks<3,>=2.2.0->llama-index-core<0.15.0,>=0.14.10->llama-index->-r requirements.txt (line 17)) (1.15.0)\n",
      "Requirement already satisfied: beautifulsoup4<5,>=4.12.3 in /opt/anaconda3/lib/python3.13/site-packages (from llama-index-readers-file<0.6,>=0.5.0->llama-index->-r requirements.txt (line 17)) (4.12.3)\n",
      "Requirement already satisfied: defusedxml>=0.7.1 in /opt/anaconda3/lib/python3.13/site-packages (from llama-index-readers-file<0.6,>=0.5.0->llama-index->-r requirements.txt (line 17)) (0.7.1)\n",
      "Requirement already satisfied: pypdf<7,>=6.1.3 in /opt/anaconda3/lib/python3.13/site-packages (from llama-index-readers-file<0.6,>=0.5.0->llama-index->-r requirements.txt (line 17)) (6.4.0)\n",
      "Requirement already satisfied: striprtf<0.0.27,>=0.0.26 in /opt/anaconda3/lib/python3.13/site-packages (from llama-index-readers-file<0.6,>=0.5.0->llama-index->-r requirements.txt (line 17)) (0.0.26)\n",
      "Requirement already satisfied: soupsieve>1.2 in /opt/anaconda3/lib/python3.13/site-packages (from beautifulsoup4<5,>=4.12.3->llama-index-readers-file<0.6,>=0.5.0->llama-index->-r requirements.txt (line 17)) (2.5)\n",
      "Requirement already satisfied: llama-index-instrumentation>=0.1.0 in /opt/anaconda3/lib/python3.13/site-packages (from llama-index-workflows!=2.9.0,<3,>=2->llama-index-core<0.15.0,>=0.14.10->llama-index->-r requirements.txt (line 17)) (0.4.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/lib/python3.13/site-packages (from pandas->datasets->-r requirements.txt (line 15)) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/lib/python3.13/site-packages (from pandas->datasets->-r requirements.txt (line 15)) (2025.2)\n",
      "Requirement already satisfied: pyproject_hooks in /opt/anaconda3/lib/python3.13/site-packages (from build>=1.0.3->chromadb->-r requirements.txt (line 9)) (1.2.0)\n",
      "Requirement already satisfied: starlette<0.51.0,>=0.40.0 in /opt/anaconda3/lib/python3.13/site-packages (from fastapi>=0.95.2->chromadb->-r requirements.txt (line 9)) (0.50.0)\n",
      "Requirement already satisfied: annotated-doc>=0.0.2 in /opt/anaconda3/lib/python3.13/site-packages (from fastapi>=0.95.2->chromadb->-r requirements.txt (line 9)) (0.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/lib/python3.13/site-packages (from Jinja2>=3.1.2->flask->-r requirements.txt (line 13)) (3.0.2)\n",
      "Requirement already satisfied: six>=1.9.0 in /opt/anaconda3/lib/python3.13/site-packages (from kubernetes>=28.1.0->chromadb->-r requirements.txt (line 9)) (1.17.0)\n",
      "Requirement already satisfied: google-auth>=1.0.1 in /opt/anaconda3/lib/python3.13/site-packages (from kubernetes>=28.1.0->chromadb->-r requirements.txt (line 9)) (2.43.0)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /opt/anaconda3/lib/python3.13/site-packages (from kubernetes>=28.1.0->chromadb->-r requirements.txt (line 9)) (1.8.0)\n",
      "Requirement already satisfied: requests-oauthlib in /opt/anaconda3/lib/python3.13/site-packages (from kubernetes>=28.1.0->chromadb->-r requirements.txt (line 9)) (2.0.0)\n",
      "Requirement already satisfied: durationpy>=0.7 in /opt/anaconda3/lib/python3.13/site-packages (from kubernetes>=28.1.0->chromadb->-r requirements.txt (line 9)) (0.10)\n",
      "Requirement already satisfied: cachetools<7.0,>=2.0.0 in /opt/anaconda3/lib/python3.13/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb->-r requirements.txt (line 9)) (5.5.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/anaconda3/lib/python3.13/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb->-r requirements.txt (line 9)) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/anaconda3/lib/python3.13/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb->-r requirements.txt (line 9)) (4.9.1)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in /opt/anaconda3/lib/python3.13/site-packages (from rsa<5,>=3.1.4->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb->-r requirements.txt (line 9)) (0.4.8)\n",
      "Requirement already satisfied: llama-cloud==0.1.35 in /opt/anaconda3/lib/python3.13/site-packages (from llama-index-indices-managed-llama-cloud>=0.4.0->llama-index->-r requirements.txt (line 17)) (0.1.35)\n",
      "Requirement already satisfied: llama-parse>=0.5.0 in /opt/anaconda3/lib/python3.13/site-packages (from llama-index-readers-llama-parse>=0.4.0->llama-index->-r requirements.txt (line 17)) (0.6.54)\n",
      "Requirement already satisfied: llama-cloud-services>=0.6.54 in /opt/anaconda3/lib/python3.13/site-packages (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index->-r requirements.txt (line 17)) (0.6.54)\n",
      "Requirement already satisfied: joblib in /opt/anaconda3/lib/python3.13/site-packages (from nltk>3.8.1->llama-index->-r requirements.txt (line 17)) (1.4.2)\n",
      "Requirement already satisfied: coloredlogs in /opt/anaconda3/lib/python3.13/site-packages (from onnxruntime>=1.14.1->chromadb->-r requirements.txt (line 9)) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in /opt/anaconda3/lib/python3.13/site-packages (from onnxruntime>=1.14.1->chromadb->-r requirements.txt (line 9)) (25.9.23)\n",
      "Requirement already satisfied: protobuf in /opt/anaconda3/lib/python3.13/site-packages (from onnxruntime>=1.14.1->chromadb->-r requirements.txt (line 9)) (5.29.3)\n",
      "Requirement already satisfied: sympy in /opt/anaconda3/lib/python3.13/site-packages (from onnxruntime>=1.14.1->chromadb->-r requirements.txt (line 9)) (1.13.3)\n",
      "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in /opt/anaconda3/lib/python3.13/site-packages (from opentelemetry-api>=1.2.0->chromadb->-r requirements.txt (line 9)) (8.5.0)\n",
      "Requirement already satisfied: zipp>=3.20 in /opt/anaconda3/lib/python3.13/site-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api>=1.2.0->chromadb->-r requirements.txt (line 9)) (3.21.0)\n",
      "Requirement already satisfied: googleapis-common-protos~=1.57 in /opt/anaconda3/lib/python3.13/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb->-r requirements.txt (line 9)) (1.72.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.39.0 in /opt/anaconda3/lib/python3.13/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb->-r requirements.txt (line 9)) (1.39.0)\n",
      "Requirement already satisfied: opentelemetry-proto==1.39.0 in /opt/anaconda3/lib/python3.13/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb->-r requirements.txt (line 9)) (1.39.0)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.60b0 in /opt/anaconda3/lib/python3.13/site-packages (from opentelemetry-sdk>=1.2.0->chromadb->-r requirements.txt (line 9)) (0.60b0)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-asgi==0.60b0 in /opt/anaconda3/lib/python3.13/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb->-r requirements.txt (line 9)) (0.60b0)\n",
      "Requirement already satisfied: opentelemetry-instrumentation==0.60b0 in /opt/anaconda3/lib/python3.13/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb->-r requirements.txt (line 9)) (0.60b0)\n",
      "Requirement already satisfied: opentelemetry-util-http==0.60b0 in /opt/anaconda3/lib/python3.13/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb->-r requirements.txt (line 9)) (0.60b0)\n",
      "Requirement already satisfied: asgiref~=3.0 in /opt/anaconda3/lib/python3.13/site-packages (from opentelemetry-instrumentation-asgi==0.60b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb->-r requirements.txt (line 9)) (3.11.0)\n",
      "Requirement already satisfied: backoff>=1.10.0 in /opt/anaconda3/lib/python3.13/site-packages (from posthog>=2.4.0->chromadb->-r requirements.txt (line 9)) (2.2.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/anaconda3/lib/python3.13/site-packages (from rich>=10.11.0->chromadb->-r requirements.txt (line 9)) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/anaconda3/lib/python3.13/site-packages (from rich>=10.11.0->chromadb->-r requirements.txt (line 9)) (2.19.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/anaconda3/lib/python3.13/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb->-r requirements.txt (line 9)) (0.1.0)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /opt/anaconda3/lib/python3.13/site-packages (from sqlalchemy[asyncio]>=1.4.49->llama-index-core<0.15.0,>=0.14.10->llama-index->-r requirements.txt (line 17)) (3.1.1)\n",
      "Requirement already satisfied: httptools>=0.6.3 in /opt/anaconda3/lib/python3.13/site-packages (from uvicorn[standard]>=0.18.3->chromadb->-r requirements.txt (line 9)) (0.7.1)\n",
      "Requirement already satisfied: uvloop>=0.15.1 in /opt/anaconda3/lib/python3.13/site-packages (from uvicorn[standard]>=0.18.3->chromadb->-r requirements.txt (line 9)) (0.22.1)\n",
      "Requirement already satisfied: watchfiles>=0.13 in /opt/anaconda3/lib/python3.13/site-packages (from uvicorn[standard]>=0.18.3->chromadb->-r requirements.txt (line 9)) (1.1.1)\n",
      "Requirement already satisfied: websockets>=10.4 in /opt/anaconda3/lib/python3.13/site-packages (from uvicorn[standard]>=0.18.3->chromadb->-r requirements.txt (line 9)) (15.0.1)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /opt/anaconda3/lib/python3.13/site-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb->-r requirements.txt (line 9)) (10.0)\n",
      "Requirement already satisfied: colorama>=0.4 in /opt/anaconda3/lib/python3.13/site-packages (from griffe->banks<3,>=2.2.0->llama-index-core<0.15.0,>=0.14.10->llama-index->-r requirements.txt (line 17)) (0.4.6)\n",
      "Requirement already satisfied: docstring-parser<1.0,>=0.16 in /opt/anaconda3/lib/python3.13/site-packages (from instructor->ragas->-r requirements.txt (line 14)) (0.17.0)\n",
      "Requirement already satisfied: pre-commit>=4.3.0 in /opt/anaconda3/lib/python3.13/site-packages (from instructor->ragas->-r requirements.txt (line 14)) (4.5.0)\n",
      "Requirement already satisfied: cfgv>=2.0.0 in /opt/anaconda3/lib/python3.13/site-packages (from pre-commit>=4.3.0->instructor->ragas->-r requirements.txt (line 14)) (3.5.0)\n",
      "Requirement already satisfied: identify>=1.0.0 in /opt/anaconda3/lib/python3.13/site-packages (from pre-commit>=4.3.0->instructor->ragas->-r requirements.txt (line 14)) (2.6.15)\n",
      "Requirement already satisfied: nodeenv>=0.11.1 in /opt/anaconda3/lib/python3.13/site-packages (from pre-commit>=4.3.0->instructor->ragas->-r requirements.txt (line 14)) (1.9.1)\n",
      "Requirement already satisfied: virtualenv>=20.10.0 in /opt/anaconda3/lib/python3.13/site-packages (from pre-commit>=4.3.0->instructor->ragas->-r requirements.txt (line 14)) (20.35.4)\n",
      "Requirement already satisfied: distlib<1,>=0.3.7 in /opt/anaconda3/lib/python3.13/site-packages (from virtualenv>=20.10.0->pre-commit>=4.3.0->instructor->ragas->-r requirements.txt (line 14)) (0.4.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/anaconda3/lib/python3.13/site-packages (from requests-oauthlib->kubernetes>=28.1.0->chromadb->-r requirements.txt (line 9)) (3.3.1)\n",
      "Requirement already satisfied: scipy>=1.7.3 in /opt/anaconda3/lib/python3.13/site-packages (from scikit-network->ragas->-r requirements.txt (line 14)) (1.15.3)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/lib/python3.13/site-packages (from sympy->onnxruntime>=1.14.1->chromadb->-r requirements.txt (line 9)) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "# Install project requirements\n",
    "#!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38bdd461-7dbf-44c4-b970-9335a9173224",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created FREE .env file successfully.\n"
     ]
    }
   ],
   "source": [
    "env_content = \"\"\"\n",
    "# ---- LLM Provider Settings ----\n",
    "LLM_PROVIDER=ollama\n",
    "OLLAMA_MODEL=llama3\n",
    "\n",
    "# ---- Embeddings ----\n",
    "# Use FREE local embeddings via Ollama\n",
    "EMBED_PROVIDER=ollama\n",
    "EMBED_MODEL=nomic-embed-text\n",
    "\n",
    "# ---- Vector Store Settings ----\n",
    "VECTOR_DB=chroma\n",
    "COLLECTION=docval\n",
    "\n",
    "# ---- API Keys ----\n",
    "# All empty because we use NO paid services\n",
    "OPENAI_API_KEY=\n",
    "MISTRAL_API_KEY=\n",
    "PINECONE_API_KEY=\n",
    "PINECONE_INDEX=docval\n",
    "\"\"\"\n",
    "\n",
    "with open(\".env\", \"w\") as f:\n",
    "    f.write(env_content)\n",
    "\n",
    "print(\"Created FREE .env file successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c0bfd6-baf9-4541-8982-f23fad9bc270",
   "metadata": {},
   "source": [
    "## 1. Imports, paths and environment\n",
    "\n",
    "In this section I import all the libraries and configure the base paths.\n",
    "\n",
    "- `os`, `argparse`, `json`, `datetime`: standard Python utilities.\n",
    "- `dotenv.load_dotenv`: loads environment variables from the `.env` file (API keys, model names, etc.).\n",
    "- LangChain loaders and tools:\n",
    "  - `Docx2txtLoader`, `PyPDFLoader`, `UnstructuredPowerPointLoader` for reading `.docx`, `.pdf`, `.pptx`.\n",
    "  - `RecursiveCharacterTextSplitter` for splitting long texts into chunks.\n",
    "  - `Document` as the basic text+metadata container.\n",
    "- Embeddings and vector stores:\n",
    "  - `OpenAIEmbeddings` to convert text chunks into vectors.\n",
    "  - `Chroma`, `FAISS`, `PineconeVectorStore` as vector DB backends.\n",
    "  - `Pinecone` client for managed Pinecone indexes.\n",
    "- Later in the file I also use:\n",
    "  - LLM chat models (OpenAI / Mistral / Ollama).\n",
    "  - `ChatPromptTemplate`, `MessagesPlaceholder`, `HumanMessage` to build prompts.\n",
    "  - `Flask` for the HTTP API.\n",
    "\n",
    "Then I define:\n",
    "- `BASE_DIR`: folder where this file lives.\n",
    "- `DATA_DIR`: directory where the raw documents to ingest are stored.\n",
    "- `VECTOR_DIR`: directory where the vector indexes (Chroma/FAISS) are stored.\n",
    "- `load_dotenv(...)`: loads `.env` from the project root so the code can read configuration from environment variables.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6a8813-4710-4b0a-b644-7520489456c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import argparse\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from langchain_community.document_loaders import Docx2txtLoader, PyPDFLoader, UnstructuredPowerPointLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_core.documents import Document\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from pinecone import Pinecone\n",
    "\n",
    "# if you have more imports (Chat models, prompts, Flask, etc.) paste them here too\n",
    "\n",
    "BASE_DIR = os.getcwd()\n",
    "DATA_DIR = os.path.join(BASE_DIR, \"data\")\n",
    "VECTOR_DIR = os.path.join(BASE_DIR, \"vector\")\n",
    "\n",
    "load_dotenv(os.path.join(BASE_DIR, \".env\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a17b38c8-c906-4c9c-a4a9-7bf2f5f9af49",
   "metadata": {},
   "source": [
    "## 2. LLM selection (`get_chat_model`)\n",
    "\n",
    "This function decides which large language model backend to use, based on environment variables.\n",
    "\n",
    "Supported providers:\n",
    "- **OpenAI GPT** (`LLM_PROVIDER=openai`): default choice, good general performance.\n",
    "- **Mistral** (`LLM_PROVIDER=mistral`): hosted Mistral models, often cheaper and EU-friendly.\n",
    "- **Llama 3 via Ollama** (`LLM_PROVIDER=ollama`): local or self-hosted option (e.g. llama3).\n",
    "\n",
    "Environment variables:\n",
    "- `LLM_PROVIDER`: which backend to use (`openai`, `mistral`, `ollama`).\n",
    "- `CHAT_MODEL`: main model name for OpenAI (e.g. `gpt-4o-mini`).\n",
    "- `MISTRAL_MODEL`: model name for Mistral (e.g. `mistral-large-latest`).\n",
    "- `OLLAMA_MODEL`: model name for Ollama (e.g. `llama3`).\n",
    "\n",
    "The idea: the rest of the code just calls `get_chat_model()` and doesnâ€™t care which provider is behind it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0faad7-3e40-4686-a2cc-469888b19ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_mistralai import ChatMistralAI\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "\n",
    "def get_chat_model():\n",
    "    provider = os.getenv(\"LLM_PROVIDER\", \"ollama\").lower()\n",
    "    # CHAT_MODEL is only used for OpenAI\n",
    "    model = os.getenv(\"CHAT_MODEL\", \"gpt-4o-mini\")\n",
    "\n",
    "    # ---- OpenAI (requires API key) ----\n",
    "    if provider == \"openai\":\n",
    "        return ChatOpenAI(\n",
    "            model=model,\n",
    "            temperature=0,\n",
    "        )\n",
    "\n",
    "    # ---- Mistral (requires API key) ----\n",
    "    if provider == \"mistral\":\n",
    "        return ChatMistralAI(\n",
    "            model=os.getenv(\"MISTRAL_MODEL\", \"mistral-large-latest\"),\n",
    "            temperature=0,\n",
    "        )\n",
    "\n",
    "    # ---- Ollama (FREE, local) ----\n",
    "    if provider == \"ollama\":\n",
    "        return ChatOllama(\n",
    "            model=os.getenv(\"OLLAMA_MODEL\", \"llama3\"),\n",
    "            temperature=0,\n",
    "        )\n",
    "\n",
    "    # ---- fallback: Ollama (free) ----\n",
    "    return ChatOllama(model=\"llama3\", temperature=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf91f6d-3884-47f3-8e5e-ee42ef4670b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/36/5wb7gt6s3h92jhx7fj1lm3hm0000gn/T/ipykernel_1612/2709603806.py:26: LangChainDeprecationWarning: The class `ChatOllama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import ChatOllama``.\n",
      "  return ChatOllama(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ChatOllama(model='llama3', temperature=0.0)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm = get_chat_model()\n",
    "llm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fff68ca-48d7-47cd-aab8-e614fb7e5e06",
   "metadata": {},
   "source": [
    "## 3. Vector store selection (`get_retriever`)\n",
    "\n",
    "This function chooses which vector database backend to use for retrieval, and returns a LangChain `retriever`.\n",
    "\n",
    "Supported vector DBs:\n",
    "- **Chroma** (`VECTOR_DB=chroma`)\n",
    "  - Local on-disk vector DB.\n",
    "  - Good for a single machine with persistence.\n",
    "- **FAISS** (`VECTOR_DB=faiss`)\n",
    "  - In-memory local index stored on disk.\n",
    "  - Good for experiments and small/medium projects.\n",
    "- **Pinecone** (`VECTOR_DB=pinecone`)\n",
    "  - Managed cloud vector DB.\n",
    "  - Good for scalable / production deployments.\n",
    "\n",
    "Steps:\n",
    "1. Create an embedding model: `OpenAIEmbeddings` with `EMBED_MODEL`.\n",
    "2. Read `VECTOR_DB` env to decide backend.\n",
    "3. For each backend:\n",
    "   - Open or create the index (Chroma/FAISS/Pinecone).\n",
    "   - Wrap it as a `retriever` with `k` nearest neighbors (and for Chroma use MMR).\n",
    "4. Return the retriever so other functions can just call `.invoke(query)` to get relevant chunks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd3b115-9d9f-4208-9e68-299a2d3d95ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import OllamaEmbeddings   # FREE local embeddings\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from pinecone import Pinecone\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ca3f16-a526-47a1-a2bc-d82bab4a4f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "\n",
    "def get_embeddings():\n",
    "    provider = os.getenv(\"EMBED_PROVIDER\", \"ollama\").lower()\n",
    "    model = os.getenv(\"EMBED_MODEL\", \"nomic-embed-text\")\n",
    "\n",
    "    print(f\"[Embedding] Provider: {provider}\")\n",
    "    print(f\"[Embedding] Model: {model}\")\n",
    "\n",
    "    if provider == \"ollama\":\n",
    "        print(\"[Embedding] Using FREE local Ollama embeddings.\")\n",
    "        return OllamaEmbeddings(model=model)\n",
    "\n",
    "    # Only used if provider=openai\n",
    "    print(\"[Embedding] Using OpenAI embeddings (PAID).\")\n",
    "    from langchain_openai import OpenAIEmbeddings\n",
    "    return OpenAIEmbeddings(model=model)\n",
    "\n",
    "\n",
    "\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from pinecone import Pinecone\n",
    "\n",
    "def get_retriever():\n",
    "    print(\"======================================\")\n",
    "    print(\"      INITIALIZING RETRIEVER\")\n",
    "    print(\"======================================\")\n",
    "\n",
    "    backend = os.getenv(\"VECTOR_DB\", \"chroma\").lower()\n",
    "    print(f\"[Retriever] Selected backend: {backend}\")\n",
    "\n",
    "    embeddings = get_embeddings()   # This will print too\n",
    "\n",
    "    # ---------- CHROMA ----------\n",
    "    if backend == \"chroma\":\n",
    "        persist_dir = os.path.join(VECTOR_DIR, \"chroma\")\n",
    "        os.makedirs(persist_dir, exist_ok=True)\n",
    "\n",
    "        print(f\"[Chroma] Persist directory: {persist_dir}\")\n",
    "        print(\"[Chroma] Loading or creating Chroma DB...\")\n",
    "\n",
    "        vectordb = Chroma(\n",
    "            collection_name=os.getenv(\"COLLECTION\", \"docval\"),\n",
    "            embedding_function=embeddings,\n",
    "            persist_directory=persist_dir,\n",
    "        )\n",
    "\n",
    "        print(\"[Chroma] Vector DB ready.\")\n",
    "        print(\"[Chroma] Returning MMR retriever.\")\n",
    "\n",
    "        return vectordb.as_retriever(\n",
    "            search_type=\"mmr\",\n",
    "            search_kwargs={\"k\": 8, \"fetch_k\": 40, \"lambda_mult\": 0.5},\n",
    "        )\n",
    "\n",
    "    # ---------- FAISS ----------\n",
    "    if backend == \"faiss\":\n",
    "        faiss_dir = os.path.join(VECTOR_DIR, \"faiss\")\n",
    "        os.makedirs(faiss_dir, exist_ok=True)\n",
    "\n",
    "        print(f\"[FAISS] Folder: {faiss_dir}\")\n",
    "\n",
    "        index_path = os.path.join(faiss_dir, \"index.faiss\")\n",
    "        print(f\"[FAISS] Index path: {index_path}\")\n",
    "\n",
    "        if os.path.exists(index_path):\n",
    "            print(\"[FAISS] Found existing FAISS index. Loading...\")\n",
    "            vectordb = FAISS.load_local(\n",
    "                faiss_dir,\n",
    "                embeddings,\n",
    "                allow_dangerous_deserialization=True,\n",
    "            )\n",
    "        else:\n",
    "            print(\"[FAISS] No index found. Creating empty FAISS index...\")\n",
    "            vectordb = FAISS.from_texts([\"\"], embeddings)\n",
    "            vectordb.save_local(faiss_dir)\n",
    "            print(\"[FAISS] Created empty FAISS index.\")\n",
    "\n",
    "        print(\"[FAISS] Returning retriever.\")\n",
    "        return vectordb.as_retriever(search_kwargs={\"k\": 8})\n",
    "\n",
    "    # ---------- PINECONE ----------\n",
    "    if backend == \"pinecone\":\n",
    "        print(\"[Pinecone] Initializing Pinecone backend...\")\n",
    "        pc = Pinecone(api_key=os.getenv(\"PINECONE_API_KEY\", \"\"))\n",
    "\n",
    "        index_name = os.getenv(\"PINECONE_INDEX\", os.getenv(\"COLLECTION\", \"docval\"))\n",
    "        print(f\"[Pinecone] Index name: {index_name}\")\n",
    "\n",
    "        dim = int(os.getenv(\"EMBED_DIM\", \"768\"))\n",
    "        print(f\"[Pinecone] Embedding dimension: {dim}\")\n",
    "\n",
    "        existing = [idx[\"name\"] for idx in pc.list_indexes()]\n",
    "        print(f\"[Pinecone] Existing indexes: {existing}\")\n",
    "\n",
    "        if index_name not in existing:\n",
    "            print(\"[Pinecone] Index not found. Creating new index...\")\n",
    "            pc.create_index(\n",
    "                name=index_name,\n",
    "                dimension=dim,\n",
    "                metric=\"cosine\",\n",
    "            )\n",
    "\n",
    "        print(\"[Pinecone] Connecting to existing index...\")\n",
    "        vectordb = PineconeVectorStore.from_existing_index(\n",
    "            index_name=index_name,\n",
    "            embedding=embeddings,\n",
    "        )\n",
    "\n",
    "        print(\"[Pinecone] Returning retriever.\")\n",
    "        return vectordb.as_retriever(search_kwargs={\"k\": 8})\n",
    "\n",
    "    # ---------- INVALID ----------\n",
    "    print(f\"[ERROR] Unsupported VECTOR_DB backend: {backend}\")\n",
    "    raise ValueError(f\"Unsupported VECTOR_DB: {backend}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2851cb1-3af7-41c0-9a69-94f63c4c0733",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- TESTING EMBEDDINGS ----\n",
      "[Embedding] Provider: ollama\n",
      "[Embedding] Model: nomic-embed-text\n",
      "[Embedding] Using FREE local Ollama embeddings.\n",
      "Embedding object: base_url='http://localhost:11434' model='nomic-embed-text' embed_instruction='passage: ' query_instruction='query: ' mirostat=None mirostat_eta=None mirostat_tau=None num_ctx=None num_gpu=None num_thread=None repeat_last_n=None repeat_penalty=None temperature=None stop=None tfs_z=None top_k=None top_p=None show_progress=False headers=None model_kwargs=None\n",
      "\n",
      "---- TESTING RETRIEVER ----\n",
      "======================================\n",
      "      INITIALIZING RETRIEVER\n",
      "======================================\n",
      "[Retriever] Selected backend: chroma\n",
      "[Embedding] Provider: ollama\n",
      "[Embedding] Model: nomic-embed-text\n",
      "[Embedding] Using FREE local Ollama embeddings.\n",
      "[Chroma] Persist directory: /Users/mona/test project of rag/test of rag/vector/chroma\n",
      "[Chroma] Loading or creating Chroma DB...\n",
      "[Chroma] Vector DB ready.\n",
      "[Chroma] Returning MMR retriever.\n",
      "Retriever object: tags=['Chroma', 'OllamaEmbeddings'] vectorstore=<langchain_chroma.vectorstores.Chroma object at 0x117efd010> search_type='mmr' search_kwargs={'k': 8, 'fetch_k': 40, 'lambda_mult': 0.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/36/5wb7gt6s3h92jhx7fj1lm3hm0000gn/T/ipykernel_12240/2121970414.py:12: LangChainDeprecationWarning: The class `OllamaEmbeddings` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaEmbeddings``.\n",
      "  return OllamaEmbeddings(model=model)\n"
     ]
    }
   ],
   "source": [
    "print(\"---- TESTING EMBEDDINGS ----\")\n",
    "emb = get_embeddings()\n",
    "print(\"Embedding object:\", emb)\n",
    "\n",
    "print(\"\\n---- TESTING RETRIEVER ----\")\n",
    "retriever = get_retriever()\n",
    "print(\"Retriever object:\", retriever)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18dfa945-4577-449f-a331-ef9482badba9",
   "metadata": {},
   "source": [
    "## 4. File loading and chunking helpers\n",
    "\n",
    "These helper functions standardize how I read and preprocess documents.\n",
    "\n",
    "- `_file_type(path)`: looks at the file extension and returns a simple type label:\n",
    "  - `.docx` â†’ `\"docx\"`\n",
    "  - `.pdf` â†’ `\"pdf\"`\n",
    "  - `.pptx` â†’ `\"pptx\"`\n",
    "  - anything else â†’ `\"other\"` (ignored)\n",
    "\n",
    "- `_load_text(abs_path, ftype)`: given an absolute path and a file type:\n",
    "  - For `docx`, uses `Docx2txtLoader` to extract text.\n",
    "  - For `pdf`, uses `PyPDFLoader` to read the pages (here I join them into one string).\n",
    "  - For `pptx`, uses `UnstructuredPowerPointLoader` and concatenates slide texts.\n",
    "\n",
    "- `_chunk_text(text)`: splits a long text into overlapping chunks using\n",
    "  `RecursiveCharacterTextSplitter` with:\n",
    "  - `chunk_size = 1000` characters\n",
    "  - `chunk_overlap = 200` characters\n",
    "  - splitting on paragraph/line/sentence boundaries when possible.\n",
    "\n",
    "The goal is to get a list of `Document` chunks ready for embeddings and retrieval.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc26b553-f9eb-4729-bd33-d1d2f5a792cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ” File helper functions LOADED.\n"
     ]
    }
   ],
   "source": [
    "def _file_type(path: str) -> str:\n",
    "    ext = os.path.splitext(path)[1].lower()\n",
    "    if ext == \".docx\":\n",
    "        return \"docx\"\n",
    "    if ext == \".pdf\":\n",
    "        return \"pdf\"\n",
    "    if ext == \".pptx\":\n",
    "        return \"pptx\"\n",
    "    return \"other\"\n",
    "\n",
    "\n",
    "def _load_text(abs_path: str, ftype: str) -> str:\n",
    "    if ftype == \"docx\":\n",
    "        docs = Docx2txtLoader(abs_path).load()\n",
    "        return docs[0].page_content\n",
    "    if ftype == \"pdf\":\n",
    "        docs = PyPDFLoader(abs_path).load()\n",
    "        return docs[0].page_content\n",
    "    if ftype == \"pptx\":\n",
    "        slides = UnstructuredPowerPointLoader(abs_path).load()\n",
    "        return \"\\n\\n\".join(d.page_content or \"\" for d in slides)\n",
    "    raise ValueError(f\"Unsupported file type: {ftype}\")\n",
    "\n",
    "\n",
    "def _chunk_text(text: str):\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000,\n",
    "        chunk_overlap=200,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \". \", \" \"],\n",
    "    )\n",
    "    return splitter.split_documents([Document(page_content=text)])\n",
    "\n",
    "print(\"âœ” File helper functions LOADED.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba3b4fb7-4a49-45bd-bacb-5966e43e3064",
   "metadata": {},
   "source": [
    "## 5. Ingestion pipeline (`ingest`)\n",
    "\n",
    "The `ingest()` function scans the `data/` directory, loads all supported documents,\n",
    "splits them into chunks, and stores them in the chosen vector database.\n",
    "\n",
    "Main steps:\n",
    "1. Ensure `DATA_DIR` exists.\n",
    "2. Create an embedding model (`OpenAIEmbeddings` with `EMBED_MODEL`).\n",
    "3. Read `VECTOR_DB` to decide backend (`chroma`, `faiss`, or `pinecone`).\n",
    "4. Walk through all files in `data/`.\n",
    "   - Skip temporary files (like `~$...` from MS Office).\n",
    "   - Detect type with `_file_type`.\n",
    "   - Use `_load_text` to read content.\n",
    "   - Use `_chunk_text` to split into smaller `Document` chunks.\n",
    "   - Attach metadata (source path, filename, type, ingestion timestamp).\n",
    "5. Insert all chunks into the selected vector store.\n",
    "6. Return a small summary: backend used and number of chunks ingested.\n",
    "\n",
    "This is the step that turns raw PDFs/DOCX/PPTX into searchable vector representations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b46352-7b52-440b-8b92-9e8101ed6ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def ingest():\n",
    "    print(\"======================================\")\n",
    "    print(\"             INGEST START\")\n",
    "    print(\"======================================\")\n",
    "\n",
    "    os.makedirs(DATA_DIR, exist_ok=True)\n",
    "\n",
    "    backend = os.getenv(\"VECTOR_DB\", \"chroma\").lower()\n",
    "    collection = os.getenv(\"COLLECTION\", \"docval\")\n",
    "\n",
    "    print(f\"[Ingest] Backend: {backend}\")\n",
    "    print(f\"[Ingest] Collection: {collection}\")\n",
    "    print(f\"[Ingest] Data folder: {DATA_DIR}\")\n",
    "\n",
    "    embeddings = get_embeddings()   # <-- FREE embeddings\n",
    "    all_docs = []\n",
    "\n",
    "    # -------------------------\n",
    "    # LOAD FILES FROM /data\n",
    "    # -------------------------\n",
    "    for root, _, files in os.walk(DATA_DIR):\n",
    "        for name in files:\n",
    "            if name.startswith(\"~$\"):\n",
    "                continue\n",
    "\n",
    "            ftype = _file_type(name)\n",
    "            if ftype == \"other\":\n",
    "                print(f\"[Skip] Unsupported file: {name}\")\n",
    "                continue\n",
    "\n",
    "            path = os.path.join(root, name)\n",
    "            print(f\"[Load] {path} (type={ftype})\")\n",
    "\n",
    "            text = _load_text(path, ftype)\n",
    "            print(f\"[Text] Loaded {len(text)} characters.\")\n",
    "\n",
    "            chunks = _chunk_text(text)\n",
    "            print(f\"[Chunks] Created {len(chunks)} chunks.\")\n",
    "\n",
    "            for d in chunks:\n",
    "                meta = {\n",
    "                    \"source_path\": path.replace(\"\\\\\", \"/\"),\n",
    "                    \"filename\": name,\n",
    "                    \"type\": ftype,\n",
    "                    \"ingested_at\": datetime.utcnow().isoformat() + \"Z\",\n",
    "                }\n",
    "                all_docs.append(\n",
    "                    Document(page_content=d.page_content, metadata=meta)\n",
    "                )\n",
    "\n",
    "    if not all_docs:\n",
    "        print(\"[Ingest] No documents found in data/.\")\n",
    "        print(\"======================================\")\n",
    "        print(\"             INGEST END\")\n",
    "        print(\"======================================\")\n",
    "        return {\"backend\": backend, \"count\": 0}\n",
    "\n",
    "    # -------------------------\n",
    "    # WRITE TO VECTOR STORE\n",
    "    # -------------------------\n",
    "    print(f\"[Ingest] Total chunks to index: {len(all_docs)}\")\n",
    "\n",
    "    if backend == \"chroma\":\n",
    "        persist_dir = os.path.join(VECTOR_DIR, \"chroma\")\n",
    "        os.makedirs(persist_dir, exist_ok=True)\n",
    "\n",
    "        print(f\"[Chroma] Persist dir: {persist_dir}\")\n",
    "        vectordb = Chroma(\n",
    "            collection_name=collection,\n",
    "            embedding_function=embeddings,\n",
    "            persist_directory=persist_dir,\n",
    "        )\n",
    "        vectordb.add_documents(all_docs)\n",
    "        print(\"[Chroma] Documents added.\")\n",
    "\n",
    "    elif backend == \"faiss\":\n",
    "        faiss_dir = os.path.join(VECTOR_DIR, \"faiss\")\n",
    "        os.makedirs(faiss_dir, exist_ok=True)\n",
    "\n",
    "        print(f\"[FAISS] Folder: {faiss_dir}\")\n",
    "\n",
    "        texts = [d.page_content for d in all_docs]\n",
    "        metas = [d.metadata for d in all_docs]\n",
    "\n",
    "        vectordb = FAISS.from_texts(texts, embeddings, metadatas=metas)\n",
    "        vectordb.save_local(faiss_dir)\n",
    "        print(\"[FAISS] Index saved.\")\n",
    "\n",
    "    elif backend == \"pinecone\":\n",
    "        pc = Pinecone(api_key=os.getenv(\"PINECONE_API_KEY\", \"\"))\n",
    "        index_name = os.getenv(\"PINECONE_INDEX\", collection)\n",
    "        print(f\"[Pinecone] Index name: {index_name}\")\n",
    "\n",
    "        dim = int(os.getenv(\"EMBED_DIM\", \"768\"))\n",
    "        print(f\"[Pinecone] Embedding dimension: {dim}\")\n",
    "\n",
    "        existing = [idx[\"name\"] for idx in pc.list_indexes()]\n",
    "        if index_name not in existing:\n",
    "            print(\"[Pinecone] Creating new index...\")\n",
    "            pc.create_index(name=index_name, dimension=dim, metric=\"cosine\")\n",
    "\n",
    "        PineconeVectorStore.from_documents(\n",
    "            documents=all_docs,\n",
    "            embedding=embeddings,\n",
    "            index_name=index_name,\n",
    "        )\n",
    "        print(\"[Pinecone] Documents saved.\")\n",
    "\n",
    "    print(\"======================================\")\n",
    "    print(\"             INGEST END\")\n",
    "    print(\"======================================\")\n",
    "    return {\"backend\": backend, \"count\": len(all_docs)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be9e1039-30de-4d7c-9fc9-d0319378175b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ignoring wrong pointing object 6 0 (offset 0)\n",
      "Ignoring wrong pointing object 8 0 (offset 0)\n",
      "Ignoring wrong pointing object 30 0 (offset 0)\n",
      "Ignoring wrong pointing object 31 0 (offset 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================\n",
      "             INGEST START\n",
      "======================================\n",
      "[Ingest] Backend: chroma\n",
      "[Ingest] Collection: docval\n",
      "[Ingest] Data folder: /Users/mona/test project of rag/test of rag/data\n",
      "[Embedding] Provider: ollama\n",
      "[Embedding] Model: nomic-embed-text\n",
      "[Embedding] Using FREE local Ollama embeddings.\n",
      "[Load] /Users/mona/test project of rag/test of rag/data/ACME_Information_Security_Policy.pdf (type=pdf)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/36/5wb7gt6s3h92jhx7fj1lm3hm0000gn/T/ipykernel_12240/1309216614.py:47: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  \"ingested_at\": datetime.utcnow().isoformat() + \"Z\",\n",
      "Ignoring wrong pointing object 6 0 (offset 0)\n",
      "Ignoring wrong pointing object 8 0 (offset 0)\n",
      "Ignoring wrong pointing object 30 0 (offset 0)\n",
      "Ignoring wrong pointing object 31 0 (offset 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Text] Loaded 1364 characters.\n",
      "[Chunks] Created 2 chunks.\n",
      "[Skip] Unsupported file: README.md\n",
      "[Load] /Users/mona/test project of rag/test of rag/data/.ipynb_checkpoints/ACME_Information_Security_Policy-checkpoint.pdf (type=pdf)\n",
      "[Text] Loaded 1364 characters.\n",
      "[Chunks] Created 2 chunks.\n",
      "[Ingest] Total chunks to index: 4\n",
      "[Chroma] Persist dir: /Users/mona/test project of rag/test of rag/vector/chroma\n",
      "[Chroma] Documents added.\n",
      "======================================\n",
      "             INGEST END\n",
      "======================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'backend': 'chroma', 'count': 4}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = ingest()\n",
    "result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8733dc39-82c6-4982-9703-c0f67a861a30",
   "metadata": {},
   "source": [
    "## 6. RAG chat (`chat`)\n",
    "\n",
    "Here I implement the main RAG question-answer function.\n",
    "\n",
    "- `_concat(docs)`: joins the `page_content` of retrieved documents into a single `context` string.\n",
    "- `chat(question)`: \n",
    "  1. Gets a `retriever` and an LLM via `get_retriever()` and `get_chat_model()`.\n",
    "  2. Builds a prompt that:\n",
    "     - Instructs the model to answer **only using the provided context**.\n",
    "     - Says to admit \"I don't know\" if the context is insufficient.\n",
    "  3. Calls the retriever with the question to get relevant chunks.\n",
    "  4. Concatenates them into a `context` string.\n",
    "  5. Sends question + context to the LLM.\n",
    "  6. Returns:\n",
    "     - `answer`: the modelâ€™s answer.\n",
    "     - `context`: the actual context used.\n",
    "     - `docs`: basic metadata (path, filename) of retrieved documents.\n",
    "\n",
    "This is the core RAG QA call used in the API and CLI.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df7df1d-9486-4a7e-a548-574671ab68d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "def _concat(docs):\n",
    "    return \"\\n\\n\".join(d.page_content for d in docs)\n",
    "\n",
    "\n",
    "def chat(question: str):\n",
    "    print(\"\\n======================================\")\n",
    "    print(\"               CHAT START\")\n",
    "    print(\"======================================\")\n",
    "\n",
    "    retriever = get_retriever()\n",
    "    llm = get_chat_model()\n",
    "\n",
    "    print(\"[Chat] Retrieving documents...\")\n",
    "    docs = retriever.get_relevant_documents(question)\n",
    "    print(f\"[Chat] Retrieved {len(docs)} docs.\")\n",
    "\n",
    "    context = _concat(docs)\n",
    "\n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\n",
    "                \"system\",\n",
    "                \"You are a strict document assistant. Answer ONLY using this context. \"\n",
    "                \"If answer is not in context, say: 'I don't know'.\"\n",
    "            ),\n",
    "            (\"system\", \"Context:\\n{context}\"),\n",
    "            (\"human\", \"{question}\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    chain = prompt | llm\n",
    "\n",
    "    print(\"[Chat] Calling LLM...\")\n",
    "    ai = chain.invoke(\n",
    "        {\n",
    "            \"context\": context,\n",
    "            \"question\": question,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    print(\"[Chat] DONE.\")\n",
    "    print(\"======================================\")\n",
    "\n",
    "    return {\n",
    "        \"answer\": ai.content,\n",
    "        \"context\": context,\n",
    "        \"docs\": [\n",
    "            {\n",
    "                \"source_path\": d.metadata.get(\"source_path\"),\n",
    "                \"filename\": d.metadata.get(\"filename\"),\n",
    "            }\n",
    "            for d in docs\n",
    "        ],\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cdaa637-d24b-4e3f-8107-b430ce56fa25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================\n",
      "               CHAT START\n",
      "======================================\n",
      "======================================\n",
      "      INITIALIZING RETRIEVER\n",
      "======================================\n",
      "[Retriever] Selected backend: chroma\n",
      "[Embedding] Provider: ollama\n",
      "[Embedding] Model: nomic-embed-text\n",
      "[Embedding] Using FREE local Ollama embeddings.\n",
      "[Chroma] Persist directory: /Users/mona/test project of rag/test of rag/vector/chroma\n",
      "[Chroma] Loading or creating Chroma DB...\n",
      "[Chroma] Vector DB ready.\n",
      "[Chroma] Returning MMR retriever.\n",
      "[Chat] Retrieving documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/36/5wb7gt6s3h92jhx7fj1lm3hm0000gn/T/ipykernel_1612/2121970414.py:12: LangChainDeprecationWarning: The class `OllamaEmbeddings` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaEmbeddings``.\n",
      "  return OllamaEmbeddings(model=model)\n",
      "/var/folders/36/5wb7gt6s3h92jhx7fj1lm3hm0000gn/T/ipykernel_1612/2154857908.py:17: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  docs = retriever.get_relevant_documents(question)\n",
      "Number of requested results 40 is greater than number of elements in index 4, updating n_results = 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Chat] Retrieved 4 docs.\n",
      "[Chat] Calling LLM...\n",
      "[Chat] DONE.\n",
      "======================================\n",
      "\n",
      "ANSWER:\n",
      " The document defines an \"Information Security Policy\".\n",
      "\n",
      "SOURCES:\n",
      " [{'source_path': '/Users/mona/test project of rag/test of rag/data/ACME_Information_Security_Policy.pdf', 'filename': 'ACME_Information_Security_Policy.pdf'}, {'source_path': '/Users/mona/test project of rag/test of rag/data/.ipynb_checkpoints/ACME_Information_Security_Policy-checkpoint.pdf', 'filename': 'ACME_Information_Security_Policy-checkpoint.pdf'}, {'source_path': '/Users/mona/test project of rag/test of rag/data/ACME_Information_Security_Policy.pdf', 'filename': 'ACME_Information_Security_Policy.pdf'}, {'source_path': '/Users/mona/test project of rag/test of rag/data/.ipynb_checkpoints/ACME_Information_Security_Policy-checkpoint.pdf', 'filename': 'ACME_Information_Security_Policy-checkpoint.pdf'}]\n"
     ]
    }
   ],
   "source": [
    "resp = chat(\"What policies does the document define?\")\n",
    "print(\"\\nANSWER:\\n\", resp[\"answer\"])\n",
    "print(\"\\nSOURCES:\\n\", resp[\"docs\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "854c7284-227d-4558-8cdd-963fb804a646",
   "metadata": {},
   "source": [
    "## 7. Answer verification (`verify`)\n",
    "\n",
    "This function checks if a generated answer is actually supported by the retrieved context.\n",
    "\n",
    "Steps:\n",
    "1. Build a prompt that shows:\n",
    "   - Question\n",
    "   - Answer\n",
    "   - Context\n",
    "2. Ask the LLM to classify the answer as one of:\n",
    "   - `supported`\n",
    "   - `partially_supported`\n",
    "   - `unsupported`\n",
    "3. Map this verdict to a numeric `confidence`:\n",
    "   - 1.0 â†’ supported\n",
    "   - 0.5 â†’ partially_supported\n",
    "   - 0.0 â†’ unsupported\n",
    "\n",
    "The goal is to detect hallucinations and give the user a confidence score.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d57ba6-97a8-42d3-a6e8-f6a13226d648",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "def verify(question: str, answer: str, context: str):\n",
    "    print(\"\\n======================================\")\n",
    "    print(\"             VERIFY START\")\n",
    "    print(\"======================================\")\n",
    "\n",
    "    llm = get_chat_model()\n",
    "\n",
    "    vprompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\n",
    "                \"system\",\n",
    "                \"You are a strict verifier. Decide if the ANSWER is supported by the CONTEXT. \"\n",
    "                \"Return ONLY one word:\\n\"\n",
    "                \"- supported\\n\"\n",
    "                \"- partially_supported\\n\"\n",
    "                \"- unsupported\"\n",
    "            ),\n",
    "            (\n",
    "                \"human\",\n",
    "                \"QUESTION:\\n{q}\\n\\nANSWER:\\n{a}\\n\\nCONTEXT:\\n{c}\\n\\nReturn the verdict:\"\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    chain = vprompt | llm\n",
    "\n",
    "    print(\"[Verify] Calling verifier model...\")\n",
    "    raw = chain.invoke({\"q\": question, \"a\": answer, \"c\": context}).content.strip().lower()\n",
    "    print(f\"[Verify] Raw model output: '{raw}'\")\n",
    "\n",
    "    # Normalize\n",
    "    if \"supported\" == raw:\n",
    "        verdict = \"supported\"\n",
    "        score = 1.0\n",
    "    elif \"partially\" in raw:\n",
    "        verdict = \"partially_supported\"\n",
    "        score = 0.5\n",
    "    else:\n",
    "        verdict = \"unsupported\"\n",
    "        score = 0.0\n",
    "\n",
    "    print(\"[Verify] Verdict:\", verdict)\n",
    "    print(\"[Verify] Confidence:\", score)\n",
    "    print(\"======================================\")\n",
    "\n",
    "    return {\"verdict\": verdict, \"confidence\": score}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e93b5cf6-2886-4b27-8004-b4d47c6a7334",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================\n",
      "               CHAT START\n",
      "======================================\n",
      "======================================\n",
      "      INITIALIZING RETRIEVER\n",
      "======================================\n",
      "[Retriever] Selected backend: chroma\n",
      "[Embedding] Provider: ollama\n",
      "[Embedding] Model: nomic-embed-text\n",
      "[Embedding] Using FREE local Ollama embeddings.\n",
      "[Chroma] Persist directory: /Users/mona/test project of rag/test of rag/vector/chroma\n",
      "[Chroma] Loading or creating Chroma DB...\n",
      "[Chroma] Vector DB ready.\n",
      "[Chroma] Returning MMR retriever.\n",
      "[Chat] Retrieving documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 40 is greater than number of elements in index 4, updating n_results = 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Chat] Retrieved 4 docs.\n",
      "[Chat] Calling LLM...\n",
      "[Chat] DONE.\n",
      "======================================\n",
      "\n",
      "======================================\n",
      "             VERIFY START\n",
      "======================================\n",
      "[Verify] Calling verifier model...\n",
      "[Verify] Raw model output: 'supported'\n",
      "[Verify] Verdict: supported\n",
      "[Verify] Confidence: 1.0\n",
      "======================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'verdict': 'supported', 'confidence': 1.0}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resp = chat(\"Does the policy include incident response?\")\n",
    "answer = resp[\"answer\"]\n",
    "context = resp[\"context\"]\n",
    "\n",
    "verify(\"Does the policy include incident response?\", answer, context)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09092517-2736-48d4-a5c1-f93a3f67e5dd",
   "metadata": {},
   "source": [
    "## 8. Requirement validation (`validate`)\n",
    "\n",
    "This function implements the \"document validation\" logic used for compliance (e.g. ISO 27001).\n",
    "\n",
    "For each requirement string:\n",
    "1. Use the retriever to fetch relevant chunks from the document.\n",
    "2. Build a context string from those chunks.\n",
    "3. Ask the LLM: \n",
    "   - Is this requirement present in the document, based on the context?\n",
    "   - Answer \"yes\" or \"no\" and give a short justification.\n",
    "4. Interpret the answer:\n",
    "   - If it starts with \"yes\" â†’ `present = True`\n",
    "   - Otherwise â†’ `present = False`\n",
    "5. Return a list of results with:\n",
    "   - `requirement`\n",
    "   - `present` (bool)\n",
    "   - `raw` (full LLM justification)\n",
    "   - `docs` (metadata of supporting chunks)\n",
    "\n",
    "This is the core feature that matches the thesis: automatic content / requirement checking.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ed74d7-79ba-4fa7-9ed1-f2bcc2631a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "def validate(requirements):\n",
    "    print(\"\\n======================================\")\n",
    "    print(\"           VALIDATION START\")\n",
    "    print(\"======================================\")\n",
    "\n",
    "    retriever = get_retriever()\n",
    "    llm = get_chat_model()\n",
    "\n",
    "    classifier = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\n",
    "                \"system\",\n",
    "                \"You are a compliance checker. \"\n",
    "                \"Decide if the REQUIREMENT is present in the document, \"\n",
    "                \"based ONLY on the given CONTEXT.\\n\"\n",
    "                \"Answer with exactly:\\n\"\n",
    "                \"  'yes: <short reason>'  or\\n\"\n",
    "                \"  'no: <short reason>'\"\n",
    "            ),\n",
    "            (\n",
    "                \"human\",\n",
    "                \"Requirement:\\n{req}\\n\\nContext:\\n{ctx}\\n\\nAnswer:\"\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    chain = classifier | llm\n",
    "    results = []\n",
    "\n",
    "    for req in requirements:\n",
    "        print(\"\\n--------------------------------------\")\n",
    "        print(f\"[Validate] Requirement: {req}\")\n",
    "\n",
    "        # get docs from retriever (NOT .invoke)\n",
    "        docs = retriever.get_relevant_documents(req)\n",
    "        print(f\"[Validate] Retrieved {len(docs)} docs\")\n",
    "\n",
    "        ctx = _concat(docs)\n",
    "\n",
    "        ans = chain.invoke({\"req\": req, \"ctx\": ctx}).content.strip()\n",
    "        present = ans.lower().startswith(\"yes\")\n",
    "\n",
    "        print(f\"[Validate] Raw answer: {ans}\")\n",
    "        print(f\"[Validate] Present? {present}\")\n",
    "\n",
    "        results.append(\n",
    "            {\n",
    "                \"requirement\": req,\n",
    "                \"present\": present,\n",
    "                \"raw\": ans,\n",
    "                \"docs\": [\n",
    "                    {\n",
    "                        \"source_path\": d.metadata.get(\"source_path\"),\n",
    "                        \"filename\": d.metadata.get(\"filename\"),\n",
    "                    }\n",
    "                    for d in docs\n",
    "                ],\n",
    "            }\n",
    "        )\n",
    "\n",
    "    print(\"\\n======================================\")\n",
    "    print(\"           VALIDATION END\")\n",
    "    print(\"======================================\")\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07127b40-71da-4c42-9f29-53c10d51264e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================\n",
      "           VALIDATION START\n",
      "======================================\n",
      "======================================\n",
      "      INITIALIZING RETRIEVER\n",
      "======================================\n",
      "[Retriever] Selected backend: chroma\n",
      "[Embedding] Provider: ollama\n",
      "[Embedding] Model: nomic-embed-text\n",
      "[Embedding] Using FREE local Ollama embeddings.\n",
      "[Chroma] Persist directory: /Users/mona/test project of rag/test of rag/vector/chroma\n",
      "[Chroma] Loading or creating Chroma DB...\n",
      "[Chroma] Vector DB ready.\n",
      "[Chroma] Returning MMR retriever.\n",
      "\n",
      "--------------------------------------\n",
      "[Validate] Requirement: The policy must include an incident response process.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 40 is greater than number of elements in index 4, updating n_results = 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Validate] Retrieved 4 docs\n",
      "[Validate] Raw answer: yes: The requirement is present in the document as it mentions \"Coordinates incident response activities\" under 3.1 CISO's responsibilities.\n",
      "[Validate] Present? True\n",
      "\n",
      "--------------------------------------\n",
      "[Validate] Requirement: The policy must define roles and responsibilities.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 40 is greater than number of elements in index 4, updating n_results = 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Validate] Retrieved 4 docs\n",
      "[Validate] Raw answer: yes: The policy defines roles and responsibilities for the CISO, IT Security Team, and Department Managers.\n",
      "[Validate] Present? True\n",
      "\n",
      "--------------------------------------\n",
      "[Validate] Requirement: The policy must describe backup and recovery.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 40 is greater than number of elements in index 4, updating n_results = 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Validate] Retrieved 4 docs\n",
      "[Validate] Raw answer: yes: The document mentions \"all stages of the information lifecycle\" which includes backup and recovery.\n",
      "[Validate] Present? True\n",
      "\n",
      "--------------------------------------\n",
      "[Validate] Requirement: The policy must define VPN usage rules.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 40 is greater than number of elements in index 4, updating n_results = 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Validate] Retrieved 4 docs\n",
      "[Validate] Raw answer: yes: The requirement is present in the document as it defines VPN usage rules under the scope section, which states \"All informa0on assets, including on-premise systems, cloud environments, laptops, mobile devices, and SaaS applica0ons.\"\n",
      "[Validate] Present? True\n",
      "\n",
      "======================================\n",
      "           VALIDATION END\n",
      "======================================\n",
      "\n",
      "REQ: The policy must include an incident response process.\n",
      "PRESENT: True\n",
      "RAW: yes: The requirement is present in the document as it mentions \"Coordinates incident response activities\" under 3.1 CISO's responsibilities.\n",
      "FILES: {'ACME_Information_Security_Policy.pdf', 'ACME_Information_Security_Policy-checkpoint.pdf'}\n",
      "\n",
      "REQ: The policy must define roles and responsibilities.\n",
      "PRESENT: True\n",
      "RAW: yes: The policy defines roles and responsibilities for the CISO, IT Security Team, and Department Managers.\n",
      "FILES: {'ACME_Information_Security_Policy.pdf', 'ACME_Information_Security_Policy-checkpoint.pdf'}\n",
      "\n",
      "REQ: The policy must describe backup and recovery.\n",
      "PRESENT: True\n",
      "RAW: yes: The document mentions \"all stages of the information lifecycle\" which includes backup and recovery.\n",
      "FILES: {'ACME_Information_Security_Policy.pdf', 'ACME_Information_Security_Policy-checkpoint.pdf'}\n",
      "\n",
      "REQ: The policy must define VPN usage rules.\n",
      "PRESENT: True\n",
      "RAW: yes: The requirement is present in the document as it defines VPN usage rules under the scope section, which states \"All informa0on assets, including on-premise systems, cloud environments, laptops, mobile devices, and SaaS applica0ons.\"\n",
      "FILES: {'ACME_Information_Security_Policy.pdf', 'ACME_Information_Security_Policy-checkpoint.pdf'}\n"
     ]
    }
   ],
   "source": [
    "reqs = [\n",
    "    \"The policy must include an incident response process.\",\n",
    "    \"The policy must define roles and responsibilities.\",\n",
    "    \"The policy must describe backup and recovery.\",\n",
    "    \"The policy must define VPN usage rules.\",\n",
    "]\n",
    "\n",
    "results = validate(reqs)\n",
    "\n",
    "for r in results:\n",
    "    print(\"\\nREQ:\", r[\"requirement\"])\n",
    "    print(\"PRESENT:\", r[\"present\"])\n",
    "    print(\"RAW:\", r[\"raw\"])\n",
    "    print(\"FILES:\", {d[\"filename\"] for d in r[\"docs\"]})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29571420-c586-44f1-bd4a-4ab549c893ac",
   "metadata": {},
   "source": [
    "## 9. Flask API (`create_app`)\n",
    "\n",
    "This function wraps the core logic into a small HTTP API:\n",
    "\n",
    "- `POST /ingest`\n",
    "  - Runs `ingest()` and returns backend + count.\n",
    "- `POST /chat`\n",
    "  - Expects JSON with `\"question\"`.\n",
    "  - Runs `chat(question)` + `verify(...)`.\n",
    "  - Returns answer, verdict, confidence, and docs.\n",
    "- `POST /validate`\n",
    "  - Expects JSON with `\"requirements\": [...]`.\n",
    "  - Returns the result of `validate(requirements)`.\n",
    "\n",
    "This makes the framework usable from a web UI or external services.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a25b93-ec8e-4744-9538-462431d02605",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask, request, jsonify\n",
    "\n",
    "def create_app():\n",
    "    app = Flask(__name__)\n",
    "\n",
    "    @app.post(\"/ingest\")\n",
    "    def _ingest():\n",
    "        res = ingest()\n",
    "        return jsonify(res)\n",
    "\n",
    "    @app.post(\"/chat\")\n",
    "    def _chat():\n",
    "        data = request.get_json(force=True) or {}\n",
    "        q = data.get(\"question\", \"\")\n",
    "\n",
    "        r = chat(q)\n",
    "        if r[\"context\"]:\n",
    "            v = verify(q, r[\"answer\"], r[\"context\"])\n",
    "        else:\n",
    "            v = {\"verdict\": \"unsupported\", \"confidence\": 0.0}\n",
    "\n",
    "        return jsonify(\n",
    "            {\n",
    "                \"answer\": r[\"answer\"],\n",
    "                \"verdict\": v[\"verdict\"],\n",
    "                \"confidence\": v[\"confidence\"],\n",
    "                \"docs\": r[\"docs\"],\n",
    "            }\n",
    "        )\n",
    "\n",
    "    @app.post(\"/validate\")\n",
    "    def _validate():\n",
    "        data = request.get_json(force=True) or {}\n",
    "        reqs = data.get(\"requirements\", [])\n",
    "        res = validate(reqs)\n",
    "        return jsonify({\"results\": res})\n",
    "\n",
    "    return app\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1561baa-b01e-4688-b497-749245d3a7e4",
   "metadata": {},
   "source": [
    "## 10. Command-line interface (`main`)\n",
    "\n",
    "The `main()` function provides a simple CLI interface with subcommands:\n",
    "\n",
    "- `docval ingest`\n",
    "  - Runs ingestion and prints a JSON summary.\n",
    "- `docval chat \"your question\"`\n",
    "  - Runs `chat()` + `verify()` and prints answer + verdict + confidence + docs.\n",
    "- `docval validate \"req1\" \"req2\" ...`\n",
    "  - Runs `validate()` on a list of requirements and prints the results.\n",
    "- `docval serve --host 0.0.0.0 --port 8000`\n",
    "  - Starts the Flask app.\n",
    "\n",
    "This is useful for quick testing without notebooks or UI.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05484348-ea7e-445a-ad02-528d598688a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    parser = argparse.ArgumentParser(prog=\"docval\")\n",
    "\n",
    "    sub = parser.add_subparsers(dest=\"cmd\")\n",
    "\n",
    "    sub.add_parser(\"ingest\")\n",
    "\n",
    "    p_chat = sub.add_parser(\"chat\")\n",
    "    p_chat.add_argument(\"question\")\n",
    "\n",
    "    p_val = sub.add_parser(\"validate\")\n",
    "    p_val.add_argument(\"requirements\", nargs=\"+\")\n",
    "\n",
    "    p_serve = sub.add_parser(\"serve\")\n",
    "    p_serve.add_argument(\"--host\", default=\"0.0.0.0\")\n",
    "    p_serve.add_argument(\"--port\", type=int, default=8000)\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    if args.cmd == \"ingest\":\n",
    "        res = ingest()\n",
    "        print(json.dumps(res, indent=2))\n",
    "\n",
    "    elif args.cmd == \"chat\":\n",
    "        r = chat(args.question)\n",
    "        if r[\"context\"]:\n",
    "            v = verify(args.question, r[\"answer\"], r[\"context\"])\n",
    "        else:\n",
    "            v = {\"verdict\": \"unsupported\", \"confidence\": 0.0}\n",
    "        print(\n",
    "            json.dumps(\n",
    "                {\n",
    "                    \"answer\": r[\"answer\"],\n",
    "                    \"verdict\": v[\"verdict\"],\n",
    "                    \"confidence\": v[\"confidence\"],\n",
    "                    \"docs\": r[\"docs\"],\n",
    "                },\n",
    "                indent=2,\n",
    "            )\n",
    "        )\n",
    "\n",
    "    elif args.cmd == \"validate\":\n",
    "        res = validate(args.requirements)\n",
    "        print(json.dumps({\"results\": res}, indent=2))\n",
    "\n",
    "    elif args.cmd == \"serve\":\n",
    "        app = create_app()\n",
    "        app.run(host=args.host, port=args.port)\n",
    "\n",
    "    else:\n",
    "        parser.print_help()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
